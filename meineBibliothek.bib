
@report{yudkowsky_intelligence_2013,
	title = {Intelligence Explosion Microeconomics},
	abstract = {I. J. Good’s thesis of the “intelligence explosion” states that a sufficiently advanced machine intelligence could build a smarter version of itself, which could in turn build an even smarter version, and that this process could continue to the point of vastly exceeding human intelligence. As Sandberg (2010) correctly notes, there have been several attempts to lay down return on investment formulas intended to represent sharp speedups in economic or technological growth, but very little attempt has been made to deal formally with Good’s intelligence explosion thesis as such. I identify the key issue as returns on cognitive reinvestment—the ability to invest more computing power, faster computers, or improved cognitive algorithms to yield cognitive labor which produces larger brains, faster brains, or better mind designs. There are many phenomena in the world which have been argued to be evidentially relevant to this question, from the observed course of hominid evolution, to Moore’s Law, to the competenceovertimeofmachinechess-playingsystems, andmanymore. Igointosome depth on some debates which then arise on how to interpret such evidence. I propose that the next step in analyzing positions on the intelligence explosion would be to formalize return on investment curves, so that each stance can formally state which possible microfoundations they hold to be falsified by historical observations. More generally,},
	institution = {Berkeley, {CA}: Machine Intelligence Research Institute},
	type = {Technical report},
	author = {Yudkowsky, Eliezer},
	date = {2013},
	keywords = {Artificial intelligence, Brain, Cognition Disorders, Computer, Computers, Hominidae, Intelligence explosion, Large, Man-Machine Systems, Moore's law, Note (document)},
	file = {Full Text PDF:/home/franz/Zotero/storage/WXT65BN5/Yudkowsky - 2013 - Intelligence Explosion Microeconomics.pdf:application/pdf}
}

@article{hadfield-menell_inverse_2017,
	title = {Inverse Reward Design},
	url = {http://arxiv.org/abs/1711.02827},
	abstract = {Autonomous agents optimize the reward function we give them. What they don't know is how hard it is for us to design a reward function that actually captures what we want. When designing the reward, we might think of some specific training scenarios, and make sure that the reward will lead to the right behavior in those scenarios. Inevitably, agents encounter new scenarios (e.g., new types of terrain) where optimizing that same reward may lead to undesired behavior. Our insight is that reward functions are merely observations about what the designer actually wants, and that they should be interpreted in the context in which they were designed. We introduce inverse reward design ({IRD}) as the problem of inferring the true objective based on the designed reward and the training {MDP}. We introduce approximate methods for solving {IRD} problems, and use their solution to plan risk-averse behavior in test {MDPs}. Empirical results suggest that this approach can help alleviate negative side effects of misspecified reward functions and mitigate reward hacking.},
	journaltitle = {{arXiv}:1711.02827 [cs]},
	author = {Hadfield-Menell, Dylan and Milli, Smitha and Abbeel, Pieter and Russell, Stuart and Dragan, Anca},
	urldate = {2019-11-15},
	date = {2017-11-07},
	eprinttype = {arxiv},
	eprint = {1711.02827},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/home/franz/Zotero/storage/NGLT4SAD/1711.html:text/html;arXiv Fulltext PDF:/home/franz/Zotero/storage/UIYRZ9AB/Hadfield-Menell et al. - 2017 - Inverse Reward Design.pdf:application/pdf}
}

@video{paul_current_2019,
	location = {San Francisco},
	title = {Current Work in {AI} Alignment},
	url = {https://www.youtube.com/watch?v=-vsYtevJ2bc},
	abstract = {Paul Christiano, a researcher at {OpenAI}, discusses the current state of research on aligning {AI} with human values: what’s happening now, what needs to happen, and how people can help. This talk covers a broad set of subgoals within alignment research, from inferring human preferences to verifying the properties of advanced systems. To learn more about effective altruism, visit effectivealtruism.org

This talk was filmed at {EA} Global 2019: San Francisco. You can learn more about these conferences at eaglobal.org},
	author = {Paul, Christiano},
	urldate = {2019-11-02},
	date = {2019},
	langid = {english}
}

@online{noauthor_ai_nodate,
	title = {{AI} Safety Myths},
	url = {https://futureoflife.org/background/aimyths/},
	abstract = {Addressing {AI} safety myths: Why do we need research to ensure {AI} remains safe and beneficial? What are the benefits and risks of artificial intelligence?},
	titleaddon = {Future of Life Institute},
	urldate = {2019-11-01},
	langid = {american},
	file = {Snapshot:/home/franz/Zotero/storage/YBAUS76E/aimyths.html:text/html}
}

@article{yudkowsky_creating_nodate,
	title = {Creating Friendly {AI} 1.0: The Analysis and Design of Benevolent Goal Architectures},
	abstract = {The goal of the field of Artificial Intelligence is to understand intelligence and create a human-equivalent or transhuman mind. Beyond this lies another question—whether the creation of this mind will benefit the world; whether the {AI} will take actions that are benevolent or malevolent, safe or uncaring, helpful or hostile.},
	pages = {282},
	author = {Yudkowsky, Eliezer},
	langid = {english},
	file = {Yudkowsky - Creating Friendly AI 1.0 The Analysis and Design .pdf:/home/franz/Zotero/storage/77AKRN9Y/Yudkowsky - Creating Friendly AI 1.0 The Analysis and Design .pdf:application/pdf}
}

@incollection{loosemore_why_2012,
	location = {Berlin, Heidelberg},
	title = {Why an Intelligence Explosion is Probable},
	isbn = {978-3-642-32560-1},
	url = {https://doi.org/10.1007/978-3-642-32560-1_5},
	series = {The Frontiers Collection},
	abstract = {The hypothesis is considered that: Once an {AI} system with roughly human-level general intelligence is created, an “intelligence explosion” involving the relatively rapid creation of increasingly more generally intelligent {AI} systems will very likely ensue, resulting in the rapid emergence of dramatically superhuman intelligences. Various arguments against this hypothesis are considered and found wanting.},
	pages = {83--98},
	booktitle = {Singularity Hypotheses: A Scientific and Philosophical Assessment},
	publisher = {Springer},
	author = {Loosemore, Richard and Goertzel, Ben},
	editor = {Eden, Amnon H. and Moor, James H. and Søraker, Johnny H. and Steinhart, Eric},
	urldate = {2019-10-30},
	date = {2012},
	langid = {english},
	doi = {10.1007/978-3-642-32560-1_5},
	keywords = {Clock Speed, Economic Growth Rate, General Intelligence, Hardware Requirement, Software Complexity}
}

@incollection{muehlhauser_intelligence_2012,
	location = {Berlin, Heidelberg},
	title = {Intelligence Explosion: Evidence and Import},
	isbn = {978-3-642-32560-1},
	series = {The Frontiers Collection},
	shorttitle = {Intelligence Explosion},
	abstract = {In this chapter we review the evidence for and against three claims: that (1) there is a substantial chance we will create human-level {AI} before 2100, that (2) if human-level {AI} is created, there is a good chance vastly superhuman {AI} will follow via an “intelligence explosion,” and that (3) an uncontrolled intelligence explosion could destroy everything we value, but a controlled intelligence explosion would benefit humanity enormously if we can achieve it. We conclude with recommendations for increasing the odds of a controlled intelligence explosion relative to an uncontrolled intelligence explosion.},
	pages = {15--42},
	booktitle = {Singularity Hypotheses: A Scientific and Philosophical Assessment},
	publisher = {Springer},
	author = {Muehlhauser, Luke and Salamon, Anna},
	editor = {Eden, Amnon H. and Moor, James H. and Søraker, Johnny H. and Steinhart, Eric},
	date = {2012},
	langid = {english},
	doi = {10.1007/978-3-642-32560-1_2},
	keywords = {Artificial Intelligence, Intelligence Software, Machine Intelligence, Optimization Power, Transcranial Magnetic Stimulation},
	file = {Muehlhauser und Salamon - 2012 - Intelligence Explosion Evidence and Import.pdf:/home/franz/Zotero/storage/CPT2XUIF/Muehlhauser und Salamon - 2012 - Intelligence Explosion Evidence and Import.pdf:application/pdf}
}

@book{hibbard_super-intelligent_2002,
	title = {Super-Intelligent Machines},
	isbn = {978-0-306-47388-3},
	abstract = {Super-Intelligent Machines combines neuroscience and computer science to analyze future intelligent machines. It describes how they will mimic the learning structures of human brains to serve billions of people via the network, and the superior level of consciousness this will give them. Whereas human learning is reinforced by self-interests, this book describes the selfless and compassionate values that must drive machine learning in order to protect human society. Technology will change life much more in the twenty-first century than it has in the twentieth, and Super-Intelligent Machines explains how that can be an advantage.},
	publisher = {Springer {US}},
	author = {Hibbard, Bill},
	date = {2002},
	langid = {english},
	doi = {10.1007/978-1-4615-0759-8},
	file = {Hibbard - 2002 - Super-Intelligent Machines.pdf:/home/franz/Zotero/storage/ZL4FVPZJ/Hibbard - 2002 - Super-Intelligent Machines.pdf:application/pdf;Snapshot:/home/franz/Zotero/storage/6FQP63XZ/9780306473883.html:text/html}
}

@inproceedings{yudkowsky_complex_2011,
	location = {Berlin, Heidelberg},
	title = {Complex Value Systems in Friendly {AI}},
	isbn = {978-3-642-22887-2},
	doi = {10.1007/978-3-642-22887-2_48},
	series = {Lecture Notes in Computer Science},
	abstract = {A common reaction to first encountering the problem statement of Friendly {AI} (”Ensure that the creation of a generally intelligent, self-improving, eventually superintelligent system realizes a positive outcome”) is to propose a simple design which allegedly suffices; or to reject the problem by replying that ”constraining” our creations is undesirable or unnecessary. This paper briefly presents some of the reasoning which suggests that Friendly {AI} is solvable, but not simply or trivially so, and that a wise strategy would be to invoke detailed learning of and inheritance from human values as a basis for further normalization and reflection.},
	pages = {388--393},
	booktitle = {Artificial General Intelligence},
	publisher = {Springer},
	author = {Yudkowsky, Eliezer},
	editor = {Schmidhuber, Jürgen and Thórisson, Kristinn R. and Looks, Moshe},
	date = {2011},
	langid = {english},
	keywords = {anthropomorphism, Friendly {AI}, machine ethics},
	file = {Yudkowsky - 2011 - Complex Value Systems in Friendly AI.pdf:/home/franz/Zotero/storage/7IKMM2NN/Yudkowsky - 2011 - Complex Value Systems in Friendly AI.pdf:application/pdf}
}

@article{kaplan_siri_2019,
	title = {Siri, Siri, in my hand: Who’s the fairest in the land? On the interpretations, illustrations, and implications of artificial intelligence},
	volume = {62},
	issn = {0007-6813},
	doi = {10.1016/j.bushor.2018.08.004},
	shorttitle = {Siri, Siri, in my hand},
	abstract = {Artificial intelligence ({AI})—defined as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation—is a topic in nearly every boardroom and at many dinner tables. Yet, despite this prominence, {AI} is still a surprisingly fuzzy concept and a lot of questions surrounding it are still open. In this article, we analyze how {AI} is different from related concepts, such as the Internet of Things and big data, and suggest that {AI} is not one monolithic term but instead needs to be seen in a more nuanced way. This can either be achieved by looking at {AI} through the lens of evolutionary stages (artificial narrow intelligence, artificial general intelligence, and artificial super intelligence) or by focusing on different types of {AI} systems (analytical {AI}, human-inspired {AI}, and humanized {AI}). Based on this classification, we show the potential and risk of {AI} using a series of case studies regarding universities, corporations, and governments. Finally, we present a framework that helps organizations think about the internal and external implications of {AI}, which we label the Three C Model of Confidence, Change, and Control.},
	number = {1},
	journaltitle = {Business Horizons},
	shortjournal = {Business Horizons},
	author = {Kaplan, Andreas and Haenlein, Michael},
	date = {2019-01-01},
	keywords = {Artificial intelligence, Big data, Deep learning, Expert systems, Internet of Things, Machine learning},
	file = {ScienceDirect Full Text PDF:/home/franz/Zotero/storage/UHADMB2F/Kaplan und Haenlein - 2019 - Siri, Siri, in my hand Who’s the fairest in the l.pdf:application/pdf;ScienceDirect Snapshot:/home/franz/Zotero/storage/U8L65GNB/S0007681318301393.html:text/html}
}

@book{grzimek_grzimeks_1979,
	title = {Grzimeks Tierleben. Band 11 Säugetiere},
	publisher = {{DTV} Deutscher Taschenbuchverlag,},
	author = {Grzimek, Bernhard},
	date = {1979}
}

@online{yudkowsky_what_2001,
	title = {What is Friendly {AI}? {\textbar} Kurzweil},
	url = {https://www.kurzweilai.net/what-is-friendly-ai},
	abstract = {“What is Friendly {AI}?” is a short introductory article to the theory of “Friendly {AI},” which attempts to answer questions such as those above. Further material on Friendly {AI} can be found at the Singularity Institute website at http://singinst.org/friendly/whatis.html, including a book-length explanation.},
	author = {Yudkowsky, Eliezer},
	urldate = {2019-10-01},
	date = {2001-03-05},
	file = {What is Friendly AI? | Kurzweil:/home/franz/Zotero/storage/P8I4KGHB/what-is-friendly-ai.html:text/html}
}

@article{vinge_coming_nodate,
	title = {The Coming Technological Singularity: How to Survive in the Post-Human Era},
	url = {https://edoras.sdsu.edu/~vinge/misc/singularity.html},
	abstract = {Within thirty years, we will have the technological
              means to create superhuman intelligence. Shortly after,
              the human era will be ended.

                   Is such progress avoidable? If not to be avoided, can
              events be guided so that we may survive?  These questions
              are investigated. Some possible answers (and some further
              dangers) are presented.},
	author = {Vinge, Vernor},
	urldate = {2019-09-07},
	langid = {english},
	file = {The Coming Technological Singularity:/home/franz/Zotero/storage/WS8XK5AD/singularity.html:text/html}
}

@incollection{muller_future_2016,
	location = {Cham},
	title = {Future Progress in Artificial Intelligence: A Survey of Expert Opinion},
	isbn = {978-3-319-26485-1},
	series = {Synthese Library},
	shorttitle = {Future Progress in Artificial Intelligence},
	abstract = {There is, in some quarters, concern about high–level machine intelligence and superintelligent {AI} coming up in a few decades, bringing with it significant risks for humanity. In other quarters, these issues are ignored or considered science fiction. We wanted to clarify what the distribution of opinions actually is, what probability the best experts currently assign to high–level machine intelligence coming up within a particular time–frame, which risks they see with that development, and how fast they see these developing. We thus designed a brief questionnaire and distributed it to four groups of experts in 2012/2013. The median estimate of respondents was for a one in two chance that high-level machine intelligence will be developed around 2040–2050, rising to a nine in ten chance by 2075. Experts expect that systems will move on to superintelligence in less than 30 years thereafter. They estimate the chance is about one in three that this development turns out to be ‘bad’ or ‘extremely bad’ for humanity.},
	pages = {555--572},
	booktitle = {Fundamental Issues of Artificial Intelligence},
	publisher = {Springer International Publishing},
	author = {Müller, Vincent C. and Bostrom, Nick},
	editor = {Müller, Vincent C.},
	date = {2016},
	langid = {english},
	doi = {10.1007/978-3-319-26485-1_33},
	keywords = {{AI}, Artificial intelligence, Expert opinion, Future of {AI}, Humanity, Intelligence explosion, Machine intelligence, Opinion poll, Progress, Singularity, Superintelligence}
}

@inproceedings{goertzel_advances_2007,
	title = {Advances in Artificial General Intelligence: Concepts, Architectures and Algorithms: Proceedings of the {AGI} Workshop 2006},
	isbn = {978-1-58603-758-1},
	shorttitle = {Advances in Artificial General Intelligence},
	abstract = {"The topic of this book the creation of software programs displaying broad, deep, human-style general intelligence is a grand and ambitious one. And yet it is far from a frivolous one: what the papers in this publication illustrate is that it is a fit and proper subject for serious science and engineering exploration. No one has yet created a software program with human-style or (even roughly) human-level general intelligence but we now have a sufficiently rich intellectual toolkit that it is possible to think about such a possibility in detail, and make serious attempts at design, analysis and engineering. possibility in detail, and make serious attempts at design, analysis and engineering. This is the situation that led to the organization of the 2006 {AGIRI} (Artificial General Intelligence Research Institute) workshop; and to the decision to publish a book from contributions by the speakers at the conference.The material presented here only scratches the surface of the {AGI}-related R\&D work that is occurring around the world at this moment. But the editors are pleased to have had the chance to be involved in organizing and presenting at least a small percentage of the contemporary progress."},
	eventtitle = {{AGI} Workshop 2006},
	publisher = {{IOS} Press},
	author = {Goertzel, Ben and Wang, Pei},
	date = {2007},
	langid = {english},
	note = {Google-Books-{ID}: t2G5srpFRhEC},
	keywords = {Computers / Intelligence ({AI}) \& Semantics}
}

@article{easterlin_worldwide_2000,
	title = {The Worldwide Standard of Living since 1800},
	volume = {14},
	issn = {0895-3309},
	url = {https://www.jstor.org/stable/2647048},
	pages = {7--26},
	number = {1},
	journaltitle = {The Journal of Economic Perspectives},
	author = {Easterlin, Richard A.},
	date = {2000}
}

@online{noauthor_eliezer_nodate,
	title = {Eliezer Yudkowsky on Intelligence Explosion - {YouTube}},
	url = {https://www.youtube.com/watch?v=D6peN9LiTWA},
	urldate = {2019-08-07},
	file = {Eliezer Yudkowsky on Intelligence Explosion - YouTube:/home/franz/Zotero/storage/EBHJDYBF/watch.html:text/html}
}

@book{bostrom_superintelligence:_2014,
	location = {Oxford},
	title = {Superintelligence: Paths, Dangers, Strategies},
	isbn = {978-0-19-967811-2},
	shorttitle = {Superintelligence},
	abstract = {A New York Times {bestsellerSuperintelligence} asks the questions: What happens when machines surpass humans in general intelligence? Will artificial agents save or destroy us? Nick Bostrom lays the foundation for understanding the future of humanity and intelligent life. The human brain has some capabilities that the brains of other animals lack. It is to these distinctive capabilities that our species owes its dominant position. If machine brains surpassed human brains in general intelligence, then this new superintelligence could become extremely powerful - possibly beyond our control. As the fate of the gorillas now depends more on humans than on the species itself, so would the fate of humankind depend on the actions of the machine superintelligence.But we have one advantage: we get to make the first move. Will it be possible to construct a seed Artificial Intelligence, to engineer initial conditions so as to make an intelligence explosion survivable? How could one achieve a controlled detonation?This profoundly ambitious and original book breaks down a vast track of difficult intellectual terrain. After an utterly engrossing journey that takes us to the frontiers of thinking about the human condition and the future of intelligent life, we find in Nick Bostrom's work nothing less than a reconceptualization of the essential task of our time.},
	pagetotal = {328},
	publisher = {Oxford University Press},
	author = {Bostrom, Nick},
	date = {2014-07-03}
}

@book{noauthor_facing_nodate,
	title = {‎Facing the Intelligence Explosion},
	url = {https://books.apple.com/us/book/facing-the-intelligence-explosion/id623915471},
	abstract = {‎Computers \& Internet · 2013},
	urldate = {2019-08-06},
	langid = {english},
	file = {Snapshot:/home/franz/Zotero/storage/DKJU5DVC/id623915471.html:text/html}
}

@book{armstrong_smarter_2014,
	title = {Smarter Than Us: The Rise of Machine Intelligence},
	shorttitle = {Smarter Than Us},
	abstract = {What happens when machines become smarter than humans? Forget lumbering Terminators. The power of artificial intelligence ({AI}) lies in its potential for superior intelligence, not physical strength or laser guns. Humans steer the future not because we’re the strongest or fastest species, but because of our cognitive advances; and once {AI} systems surpass us on this front, we’ll be handing them the steering wheel.What promises—and perils—does smarter-than-human {AI} present? Can we instruct {AI} systems to steer the future as we desire? What goals should we program into them? Stuart Armstrong’s new book navigates these questions with clarity and wit.Though an understanding of the problem is only beginning to spread, researchers from fields ranging from philosophy to computer science to economics are working together to conceive and test solutions. Are we up to the challenge?A mathematician by training, Armstrong is a Research Fellow at the Future of Humanity Institute ({FHI}) at Oxford University. His research focuses on formal decision theory, the risks and possibilities of {AI}, the long term potential for intelligent life (and the difficulties of predicting this), and anthropic (self-locating) probability.},
	pagetotal = {64},
	publisher = {Machine Intelligence Research Institute},
	author = {Armstrong, Stuart},
	date = {2014-02-01}
}

@online{noauthor_facing_nodate-1,
	title = {Facing the Intelligence Explosion},
	url = {https://intelligenceexplosion.com/},
	urldate = {2019-08-06},
	file = {Facing the Intelligence Explosion:/home/franz/Zotero/storage/KSVRN4I7/intelligenceexplosion.com.html:text/html}
}

@online{noauthor_ai_nodate-1,
	title = {{AI} Safety Myths},
	url = {https://futureoflife.org/background/aimyths/},
	abstract = {Addressing {AI} safety myths: Why do we need research to ensure {AI} remains safe and beneficial? What are the benefits and risks of artificial intelligence?},
	titleaddon = {Future of Life Institute},
	urldate = {2019-08-06},
	langid = {american},
	file = {Snapshot:/home/franz/Zotero/storage/G86HX2B3/aimyths.html:text/html}
}

@report{musk_integrated_2019,
	title = {An integrated brain-machine interface platform with thousands of channels},
	url = {http://biorxiv.org/lookup/doi/10.1101/703801},
	abstract = {Brain-machine interfaces ({BMIs}) hold promise for the restoration of sensory and motor function and the treatment of neurological disorders, but clinical {BMIs} have not yet been widely adopted, in part because modest channel counts have limited their potential. In this white paper, we describe Neuralink’s first steps toward a scalable high-bandwidth {BMI} system. We have built arrays of small and flexible electrode “threads”, with as many as 3,072 electrodes per array distributed across 96 threads. We have also built a neurosurgical robot capable of inserting six threads (192 electrodes) per minute. Each thread can be individually inserted into the brain with micron precision for avoidance of surface vasculature and targeting specific brain regions. The electrode array is packaged into a small implantable device that contains custom chips for low-power on-board amplification and digitization: the package for 3,072 channels occupies less than (23 × 18.5 × 2) mm3. A single {USB}-C cable provides full-bandwidth data streaming from the device, recording from all channels simultaneously. This system has achieved a spiking yield of up to 85.5 \% in chronically implanted electrodes. Neuralink’s approach to {BMI} has unprecedented packaging density and scalability in a clinically relevant package.},
	institution = {Neuroscience},
	type = {preprint},
	author = {Musk, Elon and {Neuralink}},
	urldate = {2019-07-19},
	date = {2019-07-17},
	langid = {english},
	doi = {10.1101/703801},
	file = {Musk und Neuralink - 2019 - An integrated brain-machine interface platform wit.pdf:/home/franz/Zotero/storage/QJ8AVP75/Musk und Neuralink - 2019 - An integrated brain-machine interface platform wit.pdf:application/pdf}
}

@article{musk_integrated_2019-1,
	title = {An integrated brain-machine interface platform with thousands of channels},
	rights = {© 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NoDerivs} 4.0 International), {CC} {BY}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/703801v2},
	doi = {10.1101/703801},
	abstract = {Brain-machine interfaces ({BMIs}) hold promise for the restoration of sensory and motor function and the treatment of neurological disorders, but clinical {BMIs} have not yet been widely adopted, in part because modest channel counts have limited their potential. In this white paper, we describe Neuralink9s first steps toward a scalable high-bandwidth {BMI} system. We have built arrays of small and flexible electrode "threads", with as many as 3,072 electrodes per array distributed across 96 threads. We have also built a neurosurgical robot capable of inserting six threads (192 electrodes) per minute. Each thread can be individually inserted into the brain with micron precision for avoidance of surface vasculature and targeting specific brain regions. The electrode array is packaged into a small implantable device that contains custom chips for low-power on-board amplification and digitization: the package for 3,072 channels occupies less than (23 x 18.5 x 2) mm$^{\textrm{3}}$. A single {USB}-C cable provides full-bandwidth data streaming from the device, recording from all channels simultaneously. This system has achieved a spiking yield of up to 85.5\% in chronically implanted electrodes. Neuralink9s approach to {BMI} has unprecedented packaging density and scalability in a clinically relevant package.},
	pages = {703801},
	journaltitle = {{bioRxiv}},
	author = {Musk, Elon and Neuralink},
	urldate = {2019-07-19},
	date = {2019-07-18},
	langid = {english},
	file = {Snapshot:/home/franz/Zotero/storage/V3SB7PR8/703801v2.html:text/html;Full Text PDF:/home/franz/Zotero/storage/NMX6T9ZV/Musk und Neuralink - 2019 - An integrated brain-machine interface platform wit.pdf:application/pdf}
}

@video{cnet_watch_2019,
	title = {Watch Elon Musk’s Neuralink presentation},
	url = {https://www.youtube.com/watch?v=lA77zsJ31nA},
	abstract = {Electric vehicles, rockets... and now brain-computer interfaces. Elon Musk's newest venture, Neuralink, aims to bridge the gap between humans and artificial intelligence by implanting tiny chips that can link up to the brain. At a press conference on July 16, Neuralink's ambitious plans were detailed for the first time, showcasing a future (a very distant future!) technology that could help people deal with brain or spinal cord injuries or controlling 3D digital avatars.

Subscribe to {CNET}: https://www.youtube.com/user/cnettv
Check out our playlists: https://www.youtube.com/user/{CNETTV}/p...
Download the new {CNET} app: https://cnet.app.link/{GWuXq}8ExzG
Like us on Facebook: https://www.facebook.com/cnet
Follow us on Twitter: https://www.twitter.com/cnet
Follow us on Instagram: http://bit.ly/2icCYYm},
	author = {{CNET}},
	urldate = {2019-07-19},
	date = {2019-07-17}
}

@online{noauthor_kunstliche_nodate,
	title = {Künstliche Intelligenz: Ethikleitlinien},
	url = {https://ec.europa.eu/commission/news/artificial-intelligence-2019-apr-08_de},
	shorttitle = {Künstliche Intelligenz},
	abstract = {Kommission treibt Arbeit an Ethikleitlinien weiter voran},
	titleaddon = {{EU}-Kommission - European Commission},
	type = {Text},
	urldate = {2019-07-15},
	langid = {german},
	file = {EthicsguidelinesfortrustworthyAI-DEpdf.pdf:/home/franz/Zotero/storage/NT424EBY/EthicsguidelinesfortrustworthyAI-DEpdf.pdf:application/pdf;Snapshot:/home/franz/Zotero/storage/54LRYVDN/artificial-intelligence-2019-apr-08_de.html:text/html}
}

@inreference{noauthor_friendly_2019,
	title = {Friendly artificial intelligence},
	url = {https://wiki.lesswrong.com/wiki/Friendly_artificial_intelligence},
	abstract = {A Friendly Artificial Intelligence (Friendly {AI}, or {FAI}) is a superintelligence (i.e., a really powerful optimization process) that produces good, beneficial outcomes rather than harmful ones. The term was coined by Eliezer Yudkowsky, so it is frequently associated with Yudkowsky's proposals for how an artificial general intelligence ({AGI}) of this sort would behave.

"Friendly {AI}" can also be used as a shorthand for Friendly {AI} theory, the field of knowledge concerned with building such an {AI}. Note that "Friendly" (with a capital "F") is being used as a term of art, referring specifically to {AIs} that promote humane values. An {FAI} need not be "friendly" in the conventional sense of being personable, compassionate, or fun to hang out with. Indeed, an {FAI} need not even be sentient.},
	booktitle = {{LessWrong}},
	date = {2019-06-29},
	langid = {english}
}

@article{cindy_mason_engineering_2015,
	title = {Engineering Kindness: Building A Machine With Compassionate Intelligence},
	url = {https://www.academia.edu/15865212/Engineering_Kindness_Building_A_Machine_With_Compassionate_Intelligence},
	shorttitle = {Engineering Kindness},
	journaltitle = {International Journal of Synthetic Emotion},
	author = {Cindy Mason and Mason, Cindy},
	date = {2015},
	file = {Engineering_Kindness_Building_A_Machine.pdf:/home/franz/Zotero/storage/I7ZMIUUJ/Engineering_Kindness_Building_A_Machine.pdf:application/pdf;Friendly artificial intelligence - Lesswrongwiki:/home/franz/Zotero/storage/57K59JGN/Friendly_artificial_intelligence.html:text/html}
}

@video{piero_scaruffi_stuart_nodate,
	title = {Stuart Russell  on "The long-term future of (Artificial) Intelligence"},
	url = {https://www.youtube.com/watch?v=mukaRhQTMP8&feature=youtu.be},
	author = {{Piero Scaruffi}},
	urldate = {2019-07-09}
}

@online{christiano_reward_2016,
	title = {The reward engineering problem - {AI} Alignment},
	url = {https://ai-alignment.com/the-reward-engineering-problem-30285c779450},
	abstract = {How can we define rewards which incentivize weak {RL} agents to behave in a desirable way?},
	titleaddon = {Medium},
	author = {Christiano, Paul},
	urldate = {2019-07-09},
	date = {2016-10-13},
	langid = {english}
}

@article{arnold_value_nodate,
	title = {Value Alignment or Misalignment -- What Will Keep Systems Accountable?},
	abstract = {Machine learning’s advances have led to new ideas about the feasibility and importance of machine ethics keeping pace, with increasing emphasis on safety, containment, and alignment. This paper addresses a recent suggestion that inverse reinforcement learning ({IRL}) could be a means to so-called “value alignment.” We critically consider how such an approach can engage the social, norm-infused nature of ethical action and outline several features of ethical appraisal that go beyond simple models of behavior, including unavoidably temporal dimensions of norms and counterfactuals. We propose that a hybrid approach for computational architectures still offers the most promising avenue for machines acting in an ethical fashion.},
	pages = {8},
	author = {Arnold, Thomas and Kasenberg, Daniel and Scheutz, Matthias},
	langid = {english},
	file = {Arnold et al. - Value Alignment or Misalignment -- What Will Keep .pdf:/home/franz/Zotero/storage/929ILADP/Arnold et al. - Value Alignment or Misalignment -- What Will Keep .pdf:application/pdf}
}

@article{erber_typen_nodate,
	title = {Typen Ku¨nstlicher Neuronaler Netze ———– Darstellung, Vergleich und Untersuchung der Anwendungsm¨oglichkeiten},
	pages = {83},
	author = {Erber, Lars},
	langid = {german},
	file = {Erber - Typen Ku¨nstlicher Neuronaler Netze ———– Darstellu.pdf:/home/franz/Zotero/storage/IMLRKKZD/Erber - Typen Ku¨nstlicher Neuronaler Netze ———– Darstellu.pdf:application/pdf}
}

@inproceedings{okal_learning_2016,
	location = {Stockholm, Sweden},
	title = {Learning socially normative robot navigation behaviors with Bayesian inverse reinforcement learning},
	isbn = {978-1-4673-8026-3},
	url = {http://ieeexplore.ieee.org/document/7487452/},
	doi = {10.1109/ICRA.2016.7487452},
	eventtitle = {2016 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	pages = {2889--2895},
	booktitle = {2016 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	publisher = {{IEEE}},
	author = {Okal, Billy and Arras, Kai O.},
	urldate = {2019-07-09},
	date = {2016-05},
	file = {Okal und Arras - 2016 - Learning socially normative robot navigation behav.pdf:/home/franz/Zotero/storage/QDGQSY99/Okal und Arras - 2016 - Learning socially normative robot navigation behav.pdf:application/pdf}
}

@online{noauthor_ai_2014,
	title = {{AI} Impacts},
	url = {https://aiimpacts.org/},
	abstract = {The {AI} Impacts project aims to improve our understanding of the long term impacts of artificial intelligence.

This site is a collection of reference pages outlining our best understanding of many relevant considerations.

You can browse a selection of pages from here, use the search or sitemap to answer specific questions, or see our blog for highlights and discussion.},
	titleaddon = {{AI} Impacts},
	urldate = {2019-07-09},
	date = {2014-12-19},
	langid = {american}
}

@inreference{noauthor_artificial_2019,
	title = {Artificial intelligence},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Artificial_intelligence&oldid=905405777},
	abstract = {In computer science,  artificial intelligence ({AI}), sometimes called machine intelligence, is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans.  Colloquially, the term "artificial intelligence" is often used to describe machines (or computers) that mimic "cognitive" functions that humans associate with the human mind, such as "learning" and "problem solving".As machines become increasingly capable, tasks considered to require "intelligence" are often removed from the definition of {AI}, a phenomenon known as the {AI} effect. A quip in Tesler's Theorem says "{AI} is whatever hasn't been done yet." For instance, optical character recognition is frequently excluded from things considered to be {AI}, having become a routine technology. Modern machine capabilities generally classified as {AI} include successfully understanding human speech, competing at the highest level in strategic game systems (such as chess and Go), autonomously operating cars, intelligent routing in content delivery networks, and military simulations.
Artificial intelligence can be classified into three different types of systems: analytical, human-inspired, and humanized artificial intelligence. Analytical {AI} has only characteristics consistent with cognitive intelligence; generating cognitive representation of the world and using learning based on past experience to inform future decisions. Human-inspired {AI} has elements from cognitive and emotional intelligence; understanding human emotions, in addition to cognitive elements, and considering them in their decision making. Humanized {AI} shows characteristics of all types of competencies (i.e., cognitive, emotional, and social intelligence), is able to be self-conscious and is self-aware in interactions with others.
Artificial intelligence was founded as an academic discipline in 1956, and in the years since has experienced several waves of optimism, followed by disappointment and the loss of funding (known as an "{AI} winter"), followed by new approaches, success and renewed funding. For most of its history, {AI} research has been divided into subfields that often fail to communicate with each other. These sub-fields are based on technical considerations, such as particular goals (e.g. "robotics" or "machine learning"), the use of particular tools ("logic" or artificial neural networks), or deep philosophical differences. Subfields have also been based on social factors (particular institutions or the work of particular researchers).The traditional problems (or goals) of {AI} research include reasoning, knowledge representation, planning, learning, natural language processing, perception and the ability to move and manipulate objects. General intelligence is among the field's long-term goals. Approaches include statistical methods, computational intelligence, and traditional symbolic {AI}. Many tools are used in {AI}, including versions of search and mathematical optimization, artificial neural networks, and methods based on statistics, probability and economics. The {AI} field draws upon computer science, information engineering, mathematics, psychology, linguistics, philosophy, and many other fields.
The field was founded on the claim that human intelligence "can be so precisely described that a machine can be made to simulate it". This raises philosophical arguments about the nature of the mind and the ethics of creating artificial beings endowed with human-like intelligence which are issues that have been explored by myth, fiction and philosophy since antiquity. Some people also consider {AI} to be a danger to humanity if it progresses unabated. Others believe that {AI}, unlike previous technological revolutions, will create a risk of mass unemployment.In the twenty-first century, {AI} techniques have experienced a resurgence following concurrent advances in computer power, large amounts of data, and theoretical understanding; and {AI} techniques have become an essential part of the technology industry, helping to solve many challenging problems in computer science, software engineering and operations research.},
	booktitle = {Wikipedia},
	urldate = {2019-07-09},
	date = {2019-07-08},
	langid = {english},
	note = {Page Version {ID}: 905405777},
	file = {Snapshot:/home/franz/Zotero/storage/P2ZAHVZN/index.html:text/html}
}

@inreference{noauthor_kunstliche_2019,
	title = {Künstliche Intelligenz},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://de.wikipedia.org/w/index.php?title=K%C3%BCnstliche_Intelligenz&oldid=189910972},
	abstract = {Künstliche Intelligenz ({KI}, auch Artifizielle Intelligenz ({AI} bzw. A. I.), englisch artificial intelligence, {AI}) ist ein Teilgebiet der Informatik, welches sich mit der Automatisierung intelligenten Verhaltens und dem Maschinellen Lernen befasst. Der Begriff ist insofern nicht eindeutig abgrenzbar, als es bereits an einer genauen Definition von „Intelligenz“ mangelt. Dennoch wird er in Forschung und Entwicklung verwendet.},
	booktitle = {Wikipedia},
	urldate = {2019-07-09},
	date = {2019-06-27},
	langid = {german},
	note = {Page Version {ID}: 189910972},
	file = {Snapshot:/home/franz/Zotero/storage/2VMQ4EUF/index.html:text/html}
}

@inreference{noauthor_ai_2019,
	title = {{AI} control problem},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=AI_control_problem&oldid=901891171},
	abstract = {In artificial intelligence ({AI}) and philosophy, the {AI} control problem is the issue of how to build a superintelligent agent that will aid its creators, and avoid inadvertently building a superintelligence that will harm its creators. Its study is motivated by the claim that the human race will have to get the control problem right "the first time", as a misprogrammed superintelligence might rationally decide to "take over the world" and refuse to permit its programmers to modify it after launch. In addition, some scholars argue that solutions to the control problem, alongside other advances in "{AI} safety engineering", might also find applications in existing non-superintelligent {AI}. Potential strategies include "capability control" (preventing an {AI} from being able to pursue harmful plans), and "motivational control" (building an {AI} that wants to be helpful).},
	booktitle = {Wikipedia},
	urldate = {2019-07-09},
	date = {2019-06-15},
	langid = {english},
	note = {Page Version {ID}: 901891171},
	file = {Snapshot:/home/franz/Zotero/storage/5MGD3ZA3/index.html:text/html}
}

@inreference{noauthor_ai_2019-1,
	title = {{AI} takeover},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=AI_takeover&oldid=903455994},
	abstract = {An {AI} takeover is a hypothetical scenario in which artificial intelligence ({AI}) becomes the dominant form of intelligence on Earth, with computers or robots effectively taking control of the planet away from the human species. Possible scenarios include replacement of the entire human workforce, takeover by a superintelligent {AI}, and the popular notion of a robot uprising. Some public figures, such as Stephen Hawking and Elon Musk, have advocated research into precautionary measures to ensure future superintelligent machines remain under human control. Robot rebellions have been a major theme throughout science fiction for many decades though the scenarios dealt with by science fiction are generally very different from those of concern to scientists.},
	booktitle = {Wikipedia},
	urldate = {2019-07-09},
	date = {2019-06-25},
	langid = {english},
	note = {Page Version {ID}: 903455994},
	file = {Snapshot:/home/franz/Zotero/storage/LJHMPX7U/index.html:text/html}
}

@online{noauthor_artificial_nodate,
	title = {Artificial Intelligence @ {MIRI}},
	url = {https://intelligence.org/},
	abstract = {{MIRI}'s artificial intelligence research is focused on developing the mathematical theory of trustworthy reasoning for advanced autonomous {AI} systems.},
	titleaddon = {Machine Intelligence Research Institute},
	urldate = {2019-07-09},
	langid = {american},
	file = {Snapshot:/home/franz/Zotero/storage/P2CT36PL/intelligence.org.html:text/html}
}

@inreference{noauthor_friendly_2019-1,
	title = {Friendly artificial intelligence},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Friendly_artificial_intelligence&oldid=903968334},
	abstract = {A friendly artificial intelligence (also friendly {AI} or {FAI}) is a hypothetical artificial general intelligence ({AGI}) that would have a positive effect on humanity. It is a part of the ethics of artificial intelligence and is closely related to machine ethics. While machine ethics is concerned with how an artificially intelligent agent should behave, friendly artificial intelligence research is focused on how to practically bring about this behaviour and ensuring it is adequately constrained.},
	booktitle = {Wikipedia},
	urldate = {2019-07-09},
	date = {2019-06-29},
	langid = {english},
	note = {Page Version {ID}: 903968334},
	file = {Snapshot:/home/franz/Zotero/storage/E8KPVCZG/index.html:text/html}
}

@inreference{noauthor_superintelligence_2019,
	title = {Superintelligence},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Superintelligence&oldid=904476345},
	abstract = {A superintelligence is a hypothetical agent that possesses intelligence far surpassing that of the brightest and most gifted human minds. "Superintelligence" may also refer to a property of problem-solving systems (e.g., superintelligent language translators or engineering assistants) whether or not these high-level intellectual competencies are embodied in agents that act in the world. A superintelligence may or may not be created by an intelligence explosion and associated with a technological singularity.
University of Oxford philosopher Nick Bostrom defines superintelligence as "any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest". The program Fritz falls short of superintelligence even though it is much better than humans at chess because Fritz cannot outperform humans in other tasks. Following Hutter and Legg, Bostrom treats superintelligence as general dominance at goal-oriented behavior, leaving open whether an artificial or human superintelligence would possess capacities such as intentionality (cf. the Chinese room argument) or first-person consciousness (cf. the hard problem of consciousness).
Technological researchers disagree about how likely present-day human intelligence is to be surpassed. Some argue that advances in artificial intelligence ({AI}) will probably result in general reasoning systems that lack human cognitive limitations. Others believe that humans will evolve or directly modify their biology so as to achieve radically greater intelligence. A number of futures studies scenarios combine elements from both of these possibilities, suggesting that humans are likely to interface with computers, or upload their minds to computers, in a way that enables substantial intelligence amplification.
Some researchers believe that superintelligence will likely follow shortly after the development of artificial general intelligence. The first generally intelligent machines are likely to immediately hold an enormous advantage in at least some forms of mental capability, including the capacity of perfect recall, a vastly superior knowledge base, and the ability to multitask in ways not possible to biological entities. This may give them the opportunity to—either as a single being or as a new species—become much more powerful than humans, and to displace them.A number of scientists and forecasters argue for prioritizing early research into the possible benefits and risks of human and machine cognitive enhancement, because of the potential social impact of such technologies.},
	booktitle = {Wikipedia},
	urldate = {2019-07-09},
	date = {2019-07-02},
	langid = {english},
	note = {Page Version {ID}: 904476345},
	file = {Snapshot:/home/franz/Zotero/storage/5SR3UQTD/index.html:text/html}
}

@inreference{noauthor_existential_2019,
	title = {Existential risk from artificial general intelligence},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Existential_risk_from_artificial_general_intelligence&oldid=903450901},
	abstract = {Existential risk from artificial general intelligence is the hypothesis that substantial progress in artificial general intelligence ({AGI}) could someday result in human extinction or some other unrecoverable global catastrophe. For instance, the human species currently dominates other species because the human brain has some distinctive capabilities that other animals lack. If {AI} surpasses humanity in general intelligence and becomes "superintelligent", then this new superintelligence could become powerful and difficult to control. Just as the fate of the mountain gorilla depends on human goodwill, so might the fate of humanity depend on the actions of a future machine superintelligence.The likelihood of this type of scenario is widely debated, and hinges in part on differing scenarios for future progress in computer science. Once the exclusive domain of science fiction, concerns about superintelligence started to become mainstream in the 2010s, and were popularized by public figures such as Stephen Hawking, Bill Gates, and Elon Musk.One source of concern is that a sudden and unexpected "intelligence explosion" might take an unprepared human race by surprise. For example, in one scenario, the first-generation computer program found able to broadly match the effectiveness of an {AI} researcher is able to rewrite its algorithms and double its speed or capabilities in six months of massively parallel processing time. The second-generation program is expected to take three months to perform a similar chunk of work, on average; in practice, doubling its own capabilities may take longer if it experiences a mini-"{AI} winter", or may be quicker if it undergoes a miniature "{AI} Spring" where ideas from the previous generation are especially easy to mutate into the next generation. In this scenario the system undergoes an unprecedently large number of generations of improvement in a short time interval, jumping from subhuman performance in many areas to superhuman performance in all relevant areas. More broadly, examples like arithmetic and Go show that progress from human-level {AI} to superhuman ability is sometimes extremely rapid.A second source of concern is that controlling a superintelligent machine (or even instilling it with human-compatible values) may be an even harder problem than naïvely supposed. Some {AGI} researchers believe that a superintelligence would naturally resist attempts to shut it off, and that preprogramming a superintelligence with complicated human values may be an extremely difficult technical task. In contrast, skeptics such as Facebook's Yann {LeCun} argue that superintelligent machines will have no desire for self-preservation.},
	booktitle = {Wikipedia},
	urldate = {2019-07-09},
	date = {2019-06-25},
	langid = {english},
	note = {Page Version {ID}: 903450901},
	file = {Snapshot:/home/franz/Zotero/storage/JX3GPRG7/index.html:text/html}
}

@inreference{noauthor_machine_2019,
	title = {Machine Intelligence Research Institute},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Machine_Intelligence_Research_Institute&oldid=897482914},
	abstract = {The Machine Intelligence Research Institute ({MIRI}), formerly the Singularity Institute for Artificial Intelligence ({SIAI}), is a non-profit organization founded in 2000 by Eliezer Yudkowsky, originally to accelerate the development of artificial intelligence, but focused since 2005 on identifying and managing the potential risks to humanity that future {AI} systems could become superintelligent.  {MIRI}'s work has focused on a friendly {AI} approach to system design and on predicting the rate of technology development.},
	booktitle = {Wikipedia},
	urldate = {2019-07-09},
	date = {2019-05-17},
	langid = {english},
	note = {Page Version {ID}: 897482914},
	file = {Snapshot:/home/franz/Zotero/storage/BIEIMQJV/index.html:text/html}
}

@online{noauthor_how_2018,
	title = {How Do We Align Artificial Intelligence with Human Values? German},
	url = {https://futureoflife.org/2018/03/29/how-do-we-align-artificial-intelligence-with-human-values-german/},
	shorttitle = {How Do We Align Artificial Intelligence with Human Values?},
	titleaddon = {Future of Life Institute},
	urldate = {2019-07-08},
	date = {2018-03-29},
	langid = {american},
	file = {Snapshot:/home/franz/Zotero/storage/SE9TM3C3/how-do-we-align-artificial-intelligence-with-human-values-german.html:text/html}
}

@article{irving_ai_2018,
	title = {{AI} safety via debate},
	abstract = {To make {AI} systems broadly useful for challenging real-world tasks, we need them to learn complex human goals and preferences. One approach to specifying complex goals asks humans to judge during training which agent behaviors are safe and useful, but this approach can fail if the task is too complicated for a human to directly judge. To help address this concern, we propose training agents via self play on a zero sum debate game. Given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information. In an analogy to complexity theory, debate with optimal play can answer any question in {PSPACE} given polynomial time judges (direct judging answers only {NP} questions). In practice, whether debate works involves empirical questions about humans and the tasks we want {AIs} to perform, plus theoretical questions about the meaning of {AI} alignment. We report results on an initial {MNIST} experiment where agents compete to convince a sparse classifier, boosting the classifier's accuracy from 59.4\% to 88.9\% given 6 pixels and from 48.2\% to 85.2\% given 4 pixels. Finally, we discuss theoretical and practical aspects of the debate model, focusing on potential weaknesses as the model scales up, and we propose future human and computer experiments to test these properties.},
	journaltitle = {{arXiv}:1805.00899 [cs, stat]},
	author = {Irving, Geoffrey and Christiano, Paul and Amodei, Dario},
	date = {2018-10-22},
	eprinttype = {arxiv},
	eprint = {1805.00899},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/franz/Zotero/storage/YEE454KP/Irving et al. - 2018 - AI safety via debate.pdf:application/pdf;arXiv.org Snapshot:/home/franz/Zotero/storage/TM2TJ57M/1805.html:text/html}
}

@article{christiano_deep_2017,
	title = {Deep reinforcement learning from human preferences},
	abstract = {For sophisticated reinforcement learning ({RL}) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex {RL} tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art {RL} systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.},
	journaltitle = {{arXiv}:1706.03741 [cs, stat]},
	author = {Christiano, Paul and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
	date = {2017-07-13},
	eprinttype = {arxiv},
	eprint = {1706.03741},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/franz/Zotero/storage/8H8799U3/Christiano et al. - 2017 - Deep reinforcement learning from human preferences.pdf:application/pdf;arXiv.org Snapshot:/home/franz/Zotero/storage/SX4SHWTY/1706.html:text/html}
}

@inproceedings{omohundro_basic_2008,
	title = {The Basic {AI} Drives},
	volume = {171},
	abstract = {One might imagine that {AI} systems with harmless goals will be harmless. This paper instead shows that intelligent systems will need to be carefully designed to prevent them from behaving in harmful ways. We identify a number of “drives” that will appear in sufficiently advanced {AI} systems of any design. We call them drives because they are tendencies which will be present unless explicitly counteracted. We start by showing that goal-seeking systems will have drives to model their own operation and to improve themselves. We then show that self-improving systems will be driven to clarify their goals and represent them as economic utility functions. They will also strive for their actions to approximate rational economic behavior. This will lead almost all systems to protect their utility functions from modification and their utility measurement systems from corruption. We also discuss some exceptional systems which will want to modify their utility functions. We next discuss the drive toward self-protection which causes systems try to prevent themselves from being harmed. Finally we examine drives toward the acquisition of resources and toward their efficient utilization. We end with a discussion of how to incorporate these insights in designing intelligent technology which will lead to a positive future for humanity.},
	eventtitle = {First {AGI} Conference},
	author = {Omohundro, Stephen M.},
	date = {2008},
	keywords = {Approximation algorithm, Artificial intelligence, Entity, Forge, Friendly artificial intelligence, Intelligent agent, Iteration, Social structure, System of measurement}
}