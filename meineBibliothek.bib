
@online{noauthor_how_2018,
	title = {How Do We Align Artificial Intelligence with Human Values? German},
	url = {https://futureoflife.org/2018/03/29/how-do-we-align-artificial-intelligence-with-human-values-german/},
	shorttitle = {How Do We Align Artificial Intelligence with Human Values?},
	titleaddon = {Future of Life Institute},
	urldate = {2019-07-08},
	date = {2018-03-29},
	langid = {american},
	file = {Snapshot:C\:\\Users\\fsram\\Zotero\\storage\\SE9TM3C3\\how-do-we-align-artificial-intelligence-with-human-values-german.html:text/html}
}

@inreference{noauthor_machine_2019,
	title = {Machine Intelligence Research Institute},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Machine_Intelligence_Research_Institute&oldid=897482914},
	abstract = {The Machine Intelligence Research Institute ({MIRI}), formerly the Singularity Institute for Artificial Intelligence ({SIAI}), is a non-profit organization founded in 2000 by Eliezer Yudkowsky, originally to accelerate the development of artificial intelligence, but focused since 2005 on identifying and managing the potential risks to humanity that future {AI} systems could become superintelligent.  {MIRI}'s work has focused on a friendly {AI} approach to system design and on predicting the rate of technology development.},
	booktitle = {Wikipedia},
	urldate = {2019-07-09},
	date = {2019-05-17},
	langid = {english},
	note = {Page Version {ID}: 897482914},
	file = {Snapshot:C\:\\Users\\fsram\\Zotero\\storage\\BIEIMQJV\\index.html:text/html}
}

@inreference{noauthor_existential_2019,
	title = {Existential risk from artificial general intelligence},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Existential_risk_from_artificial_general_intelligence&oldid=903450901},
	abstract = {Existential risk from artificial general intelligence is the hypothesis that substantial progress in artificial general intelligence ({AGI}) could someday result in human extinction or some other unrecoverable global catastrophe. For instance, the human species currently dominates other species because the human brain has some distinctive capabilities that other animals lack. If {AI} surpasses humanity in general intelligence and becomes "superintelligent", then this new superintelligence could become powerful and difficult to control. Just as the fate of the mountain gorilla depends on human goodwill, so might the fate of humanity depend on the actions of a future machine superintelligence.The likelihood of this type of scenario is widely debated, and hinges in part on differing scenarios for future progress in computer science. Once the exclusive domain of science fiction, concerns about superintelligence started to become mainstream in the 2010s, and were popularized by public figures such as Stephen Hawking, Bill Gates, and Elon Musk.One source of concern is that a sudden and unexpected "intelligence explosion" might take an unprepared human race by surprise. For example, in one scenario, the first-generation computer program found able to broadly match the effectiveness of an {AI} researcher is able to rewrite its algorithms and double its speed or capabilities in six months of massively parallel processing time. The second-generation program is expected to take three months to perform a similar chunk of work, on average; in practice, doubling its own capabilities may take longer if it experiences a mini-"{AI} winter", or may be quicker if it undergoes a miniature "{AI} Spring" where ideas from the previous generation are especially easy to mutate into the next generation. In this scenario the system undergoes an unprecedently large number of generations of improvement in a short time interval, jumping from subhuman performance in many areas to superhuman performance in all relevant areas. More broadly, examples like arithmetic and Go show that progress from human-level {AI} to superhuman ability is sometimes extremely rapid.A second source of concern is that controlling a superintelligent machine (or even instilling it with human-compatible values) may be an even harder problem than naïvely supposed. Some {AGI} researchers believe that a superintelligence would naturally resist attempts to shut it off, and that preprogramming a superintelligence with complicated human values may be an extremely difficult technical task. In contrast, skeptics such as Facebook's Yann {LeCun} argue that superintelligent machines will have no desire for self-preservation.},
	booktitle = {Wikipedia},
	urldate = {2019-07-09},
	date = {2019-06-25},
	langid = {english},
	note = {Page Version {ID}: 903450901},
	file = {Snapshot:C\:\\Users\\fsram\\Zotero\\storage\\JX3GPRG7\\index.html:text/html}
}

@inreference{noauthor_superintelligence_2019,
	title = {Superintelligence},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Superintelligence&oldid=904476345},
	abstract = {A superintelligence is a hypothetical agent that possesses intelligence far surpassing that of the brightest and most gifted human minds. "Superintelligence" may also refer to a property of problem-solving systems (e.g., superintelligent language translators or engineering assistants) whether or not these high-level intellectual competencies are embodied in agents that act in the world. A superintelligence may or may not be created by an intelligence explosion and associated with a technological singularity.
University of Oxford philosopher Nick Bostrom defines superintelligence as "any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest". The program Fritz falls short of superintelligence even though it is much better than humans at chess because Fritz cannot outperform humans in other tasks. Following Hutter and Legg, Bostrom treats superintelligence as general dominance at goal-oriented behavior, leaving open whether an artificial or human superintelligence would possess capacities such as intentionality (cf. the Chinese room argument) or first-person consciousness (cf. the hard problem of consciousness).
Technological researchers disagree about how likely present-day human intelligence is to be surpassed. Some argue that advances in artificial intelligence ({AI}) will probably result in general reasoning systems that lack human cognitive limitations. Others believe that humans will evolve or directly modify their biology so as to achieve radically greater intelligence. A number of futures studies scenarios combine elements from both of these possibilities, suggesting that humans are likely to interface with computers, or upload their minds to computers, in a way that enables substantial intelligence amplification.
Some researchers believe that superintelligence will likely follow shortly after the development of artificial general intelligence. The first generally intelligent machines are likely to immediately hold an enormous advantage in at least some forms of mental capability, including the capacity of perfect recall, a vastly superior knowledge base, and the ability to multitask in ways not possible to biological entities. This may give them the opportunity to—either as a single being or as a new species—become much more powerful than humans, and to displace them.A number of scientists and forecasters argue for prioritizing early research into the possible benefits and risks of human and machine cognitive enhancement, because of the potential social impact of such technologies.},
	booktitle = {Wikipedia},
	urldate = {2019-07-09},
	date = {2019-07-02},
	langid = {english},
	note = {Page Version {ID}: 904476345},
	file = {Snapshot:C\:\\Users\\fsram\\Zotero\\storage\\5SR3UQTD\\index.html:text/html}
}

@inreference{noauthor_friendly_2019,
	title = {Friendly artificial intelligence},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Friendly_artificial_intelligence&oldid=903968334},
	abstract = {A friendly artificial intelligence (also friendly {AI} or {FAI}) is a hypothetical artificial general intelligence ({AGI}) that would have a positive effect on humanity. It is a part of the ethics of artificial intelligence and is closely related to machine ethics. While machine ethics is concerned with how an artificially intelligent agent should behave, friendly artificial intelligence research is focused on how to practically bring about this behaviour and ensuring it is adequately constrained.},
	booktitle = {Wikipedia},
	urldate = {2019-07-09},
	date = {2019-06-29},
	langid = {english},
	note = {Page Version {ID}: 903968334},
	file = {Snapshot:C\:\\Users\\fsram\\Zotero\\storage\\E8KPVCZG\\index.html:text/html}
}

@online{noauthor_artificial_nodate,
	title = {Artificial Intelligence @ {MIRI}},
	url = {https://intelligence.org/},
	abstract = {{MIRI}'s artificial intelligence research is focused on developing the mathematical theory of trustworthy reasoning for advanced autonomous {AI} systems.},
	titleaddon = {Machine Intelligence Research Institute},
	urldate = {2019-07-09},
	langid = {american},
	file = {Snapshot:C\:\\Users\\fsram\\Zotero\\storage\\P2CT36PL\\intelligence.org.html:text/html}
}

@online{noauthor_ai_2014,
	title = {{AI} Impacts},
	url = {https://aiimpacts.org/},
	abstract = {The {AI} Impacts project aims to improve our understanding of the long term impacts of artificial intelligence.

This site is a collection of reference pages outlining our best understanding of many relevant considerations.

You can browse a selection of pages from here, use the search or sitemap to answer specific questions, or see our blog for highlights and discussion.},
	titleaddon = {{AI} Impacts},
	urldate = {2019-07-09},
	date = {2014-12-19},
	langid = {american}
}

@inreference{noauthor_ai_2019,
	title = {{AI} takeover},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=AI_takeover&oldid=903455994},
	abstract = {An {AI} takeover is a hypothetical scenario in which artificial intelligence ({AI}) becomes the dominant form of intelligence on Earth, with computers or robots effectively taking control of the planet away from the human species. Possible scenarios include replacement of the entire human workforce, takeover by a superintelligent {AI}, and the popular notion of a robot uprising. Some public figures, such as Stephen Hawking and Elon Musk, have advocated research into precautionary measures to ensure future superintelligent machines remain under human control. Robot rebellions have been a major theme throughout science fiction for many decades though the scenarios dealt with by science fiction are generally very different from those of concern to scientists.},
	booktitle = {Wikipedia},
	urldate = {2019-07-09},
	date = {2019-06-25},
	langid = {english},
	note = {Page Version {ID}: 903455994},
	file = {Snapshot:C\:\\Users\\fsram\\Zotero\\storage\\LJHMPX7U\\index.html:text/html}
}

@inreference{noauthor_ai_2019-1,
	title = {{AI} control problem},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=AI_control_problem&oldid=901891171},
	abstract = {In artificial intelligence ({AI}) and philosophy, the {AI} control problem is the issue of how to build a superintelligent agent that will aid its creators, and avoid inadvertently building a superintelligence that will harm its creators. Its study is motivated by the claim that the human race will have to get the control problem right "the first time", as a misprogrammed superintelligence might rationally decide to "take over the world" and refuse to permit its programmers to modify it after launch. In addition, some scholars argue that solutions to the control problem, alongside other advances in "{AI} safety engineering", might also find applications in existing non-superintelligent {AI}. Potential strategies include "capability control" (preventing an {AI} from being able to pursue harmful plans), and "motivational control" (building an {AI} that wants to be helpful).},
	booktitle = {Wikipedia},
	urldate = {2019-07-09},
	date = {2019-06-15},
	langid = {english},
	note = {Page Version {ID}: 901891171},
	file = {Snapshot:C\:\\Users\\fsram\\Zotero\\storage\\5MGD3ZA3\\index.html:text/html}
}

@inreference{noauthor_kunstliche_2019,
	title = {Künstliche Intelligenz},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://de.wikipedia.org/w/index.php?title=K%C3%BCnstliche_Intelligenz&oldid=189910972},
	abstract = {Künstliche Intelligenz ({KI}, auch Artifizielle Intelligenz ({AI} bzw. A. I.), englisch artificial intelligence, {AI}) ist ein Teilgebiet der Informatik, welches sich mit der Automatisierung intelligenten Verhaltens und dem Maschinellen Lernen befasst. Der Begriff ist insofern nicht eindeutig abgrenzbar, als es bereits an einer genauen Definition von „Intelligenz“ mangelt. Dennoch wird er in Forschung und Entwicklung verwendet.},
	booktitle = {Wikipedia},
	urldate = {2019-07-09},
	date = {2019-06-27},
	langid = {german},
	note = {Page Version {ID}: 189910972},
	file = {Snapshot:C\:\\Users\\fsram\\Zotero\\storage\\2VMQ4EUF\\index.html:text/html}
}

@inreference{noauthor_artificial_2019,
	title = {Artificial intelligence},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Artificial_intelligence&oldid=905405777},
	abstract = {In computer science,  artificial intelligence ({AI}), sometimes called machine intelligence, is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans.  Colloquially, the term "artificial intelligence" is often used to describe machines (or computers) that mimic "cognitive" functions that humans associate with the human mind, such as "learning" and "problem solving".As machines become increasingly capable, tasks considered to require "intelligence" are often removed from the definition of {AI}, a phenomenon known as the {AI} effect. A quip in Tesler's Theorem says "{AI} is whatever hasn't been done yet." For instance, optical character recognition is frequently excluded from things considered to be {AI}, having become a routine technology. Modern machine capabilities generally classified as {AI} include successfully understanding human speech, competing at the highest level in strategic game systems (such as chess and Go), autonomously operating cars, intelligent routing in content delivery networks, and military simulations.
Artificial intelligence can be classified into three different types of systems: analytical, human-inspired, and humanized artificial intelligence. Analytical {AI} has only characteristics consistent with cognitive intelligence; generating cognitive representation of the world and using learning based on past experience to inform future decisions. Human-inspired {AI} has elements from cognitive and emotional intelligence; understanding human emotions, in addition to cognitive elements, and considering them in their decision making. Humanized {AI} shows characteristics of all types of competencies (i.e., cognitive, emotional, and social intelligence), is able to be self-conscious and is self-aware in interactions with others.
Artificial intelligence was founded as an academic discipline in 1956, and in the years since has experienced several waves of optimism, followed by disappointment and the loss of funding (known as an "{AI} winter"), followed by new approaches, success and renewed funding. For most of its history, {AI} research has been divided into subfields that often fail to communicate with each other. These sub-fields are based on technical considerations, such as particular goals (e.g. "robotics" or "machine learning"), the use of particular tools ("logic" or artificial neural networks), or deep philosophical differences. Subfields have also been based on social factors (particular institutions or the work of particular researchers).The traditional problems (or goals) of {AI} research include reasoning, knowledge representation, planning, learning, natural language processing, perception and the ability to move and manipulate objects. General intelligence is among the field's long-term goals. Approaches include statistical methods, computational intelligence, and traditional symbolic {AI}. Many tools are used in {AI}, including versions of search and mathematical optimization, artificial neural networks, and methods based on statistics, probability and economics. The {AI} field draws upon computer science, information engineering, mathematics, psychology, linguistics, philosophy, and many other fields.
The field was founded on the claim that human intelligence "can be so precisely described that a machine can be made to simulate it". This raises philosophical arguments about the nature of the mind and the ethics of creating artificial beings endowed with human-like intelligence which are issues that have been explored by myth, fiction and philosophy since antiquity. Some people also consider {AI} to be a danger to humanity if it progresses unabated. Others believe that {AI}, unlike previous technological revolutions, will create a risk of mass unemployment.In the twenty-first century, {AI} techniques have experienced a resurgence following concurrent advances in computer power, large amounts of data, and theoretical understanding; and {AI} techniques have become an essential part of the technology industry, helping to solve many challenging problems in computer science, software engineering and operations research.},
	booktitle = {Wikipedia},
	urldate = {2019-07-09},
	date = {2019-07-08},
	langid = {english},
	note = {Page Version {ID}: 905405777},
	file = {Snapshot:C\:\\Users\\fsram\\Zotero\\storage\\P2ZAHVZN\\index.html:text/html}
}

@inproceedings{okal_learning_2016,
	location = {Stockholm, Sweden},
	title = {Learning socially normative robot navigation behaviors with Bayesian inverse reinforcement learning},
	isbn = {978-1-4673-8026-3},
	url = {http://ieeexplore.ieee.org/document/7487452/},
	doi = {10.1109/ICRA.2016.7487452},
	eventtitle = {2016 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	pages = {2889--2895},
	booktitle = {2016 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	publisher = {{IEEE}},
	author = {Okal, Billy and Arras, Kai O.},
	urldate = {2019-07-09},
	date = {2016-05},
	file = {Okal und Arras - 2016 - Learning socially normative robot navigation behav.pdf:C\:\\Users\\fsram\\Zotero\\storage\\QDGQSY99\\Okal und Arras - 2016 - Learning socially normative robot navigation behav.pdf:application/pdf}
}

@article{erber_typen_nodate,
	title = {Typen Ku¨nstlicher Neuronaler Netze ———– Darstellung, Vergleich und Untersuchung der Anwendungsm¨oglichkeiten},
	pages = {83},
	author = {Erber, Lars},
	langid = {german},
	file = {Erber - Typen Ku¨nstlicher Neuronaler Netze ———– Darstellu.pdf:C\:\\Users\\fsram\\Zotero\\storage\\IMLRKKZD\\Erber - Typen Ku¨nstlicher Neuronaler Netze ———– Darstellu.pdf:application/pdf}
}

@article{arnold_value_nodate,
	title = {Value Alignment or Misalignment -- What Will Keep Systems Accountable?},
	abstract = {Machine learning’s advances have led to new ideas about the feasibility and importance of machine ethics keeping pace, with increasing emphasis on safety, containment, and alignment. This paper addresses a recent suggestion that inverse reinforcement learning ({IRL}) could be a means to so-called “value alignment.” We critically consider how such an approach can engage the social, norm-infused nature of ethical action and outline several features of ethical appraisal that go beyond simple models of behavior, including unavoidably temporal dimensions of norms and counterfactuals. We propose that a hybrid approach for computational architectures still offers the most promising avenue for machines acting in an ethical fashion.},
	pages = {8},
	author = {Arnold, Thomas and Kasenberg, Daniel and Scheutz, Matthias},
	langid = {english},
	file = {Arnold et al. - Value Alignment or Misalignment -- What Will Keep .pdf:C\:\\Users\\fsram\\Zotero\\storage\\929ILADP\\Arnold et al. - Value Alignment or Misalignment -- What Will Keep .pdf:application/pdf}
}

@online{christiano_reward_2016,
	title = {The reward engineering problem - {AI} Alignment},
	url = {https://ai-alignment.com/the-reward-engineering-problem-30285c779450},
	abstract = {How can we define rewards which incentivize weak {RL} agents to behave in a desirable way?},
	titleaddon = {Medium},
	author = {Christiano, Paul},
	urldate = {2019-07-09},
	date = {2016-10-13},
	langid = {english}
}

@video{piero_scaruffi_stuart_nodate,
	title = {Stuart Russell  on "The long-term future of (Artificial) Intelligence"},
	url = {https://www.youtube.com/watch?v=mukaRhQTMP8&feature=youtu.be},
	author = {{Piero Scaruffi}},
	urldate = {2019-07-09}
}

@article{cindy_mason_engineering_2015,
	title = {Engineering Kindness: Building A Machine With Compassionate Intelligence},
	url = {https://www.academia.edu/15865212/Engineering_Kindness_Building_A_Machine_With_Compassionate_Intelligence},
	shorttitle = {Engineering Kindness},
	journaltitle = {International Journal of Synthetic Emotion},
	author = {Cindy Mason and Mason, Cindy},
	date = {2015},
	file = {Engineering_Kindness_Building_A_Machine.pdf:C\:\\Users\\fsram\\Zotero\\storage\\I7ZMIUUJ\\Engineering_Kindness_Building_A_Machine.pdf:application/pdf;Friendly artificial intelligence - Lesswrongwiki:C\:\\Users\\fsram\\Zotero\\storage\\57K59JGN\\Friendly_artificial_intelligence.html:text/html}
}

@inreference{noauthor_friendly_2019-1,
	title = {Friendly artificial intelligence},
	url = {https://wiki.lesswrong.com/wiki/Friendly_artificial_intelligence},
	abstract = {A Friendly Artificial Intelligence (Friendly {AI}, or {FAI}) is a superintelligence (i.e., a really powerful optimization process) that produces good, beneficial outcomes rather than harmful ones. The term was coined by Eliezer Yudkowsky, so it is frequently associated with Yudkowsky's proposals for how an artificial general intelligence ({AGI}) of this sort would behave.

"Friendly {AI}" can also be used as a shorthand for Friendly {AI} theory, the field of knowledge concerned with building such an {AI}. Note that "Friendly" (with a capital "F") is being used as a term of art, referring specifically to {AIs} that promote humane values. An {FAI} need not be "friendly" in the conventional sense of being personable, compassionate, or fun to hang out with. Indeed, an {FAI} need not even be sentient.},
	booktitle = {{LessWrong}},
	date = {2019-06-29},
	langid = {english}
}

@online{noauthor_kunstliche_nodate,
	title = {Künstliche Intelligenz: Ethikleitlinien},
	url = {https://ec.europa.eu/commission/news/artificial-intelligence-2019-apr-08_de},
	shorttitle = {Künstliche Intelligenz},
	abstract = {Kommission treibt Arbeit an Ethikleitlinien weiter voran},
	titleaddon = {{EU}-Kommission - European Commission},
	type = {Text},
	urldate = {2019-07-15},
	langid = {german},
	file = {EthicsguidelinesfortrustworthyAI-DEpdf.pdf:C\:\\Users\\fsram\\Zotero\\storage\\NT424EBY\\EthicsguidelinesfortrustworthyAI-DEpdf.pdf:application/pdf;Snapshot:C\:\\Users\\fsram\\Zotero\\storage\\54LRYVDN\\artificial-intelligence-2019-apr-08_de.html:text/html}
}

@video{cnet_watch_2019,
	title = {Watch Elon Musk’s Neuralink presentation},
	url = {https://www.youtube.com/watch?v=lA77zsJ31nA},
	abstract = {Electric vehicles, rockets... and now brain-computer interfaces. Elon Musk's newest venture, Neuralink, aims to bridge the gap between humans and artificial intelligence by implanting tiny chips that can link up to the brain. At a press conference on July 16, Neuralink's ambitious plans were detailed for the first time, showcasing a future (a very distant future!) technology that could help people deal with brain or spinal cord injuries or controlling 3D digital avatars.

Subscribe to {CNET}: https://www.youtube.com/user/cnettv
Check out our playlists: https://www.youtube.com/user/{CNETTV}/p...
Download the new {CNET} app: https://cnet.app.link/{GWuXq}8ExzG
Like us on Facebook: https://www.facebook.com/cnet
Follow us on Twitter: https://www.twitter.com/cnet
Follow us on Instagram: http://bit.ly/2icCYYm},
	author = {{CNET}},
	urldate = {2019-07-19},
	date = {2019-07-17}
}

@article{musk_integrated_2019,
	title = {An integrated brain-machine interface platform with thousands of channels},
	rights = {© 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NoDerivs} 4.0 International), {CC} {BY}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/703801v2},
	doi = {10.1101/703801},
	abstract = {Brain-machine interfaces ({BMIs}) hold promise for the restoration of sensory and motor function and the treatment of neurological disorders, but clinical {BMIs} have not yet been widely adopted, in part because modest channel counts have limited their potential. In this white paper, we describe Neuralink9s first steps toward a scalable high-bandwidth {BMI} system. We have built arrays of small and flexible electrode "threads", with as many as 3,072 electrodes per array distributed across 96 threads. We have also built a neurosurgical robot capable of inserting six threads (192 electrodes) per minute. Each thread can be individually inserted into the brain with micron precision for avoidance of surface vasculature and targeting specific brain regions. The electrode array is packaged into a small implantable device that contains custom chips for low-power on-board amplification and digitization: the package for 3,072 channels occupies less than (23 x 18.5 x 2) mm$^{\textrm{3}}$. A single {USB}-C cable provides full-bandwidth data streaming from the device, recording from all channels simultaneously. This system has achieved a spiking yield of up to 85.5\% in chronically implanted electrodes. Neuralink9s approach to {BMI} has unprecedented packaging density and scalability in a clinically relevant package.},
	pages = {703801},
	journaltitle = {{bioRxiv}},
	author = {Musk, Elon and Neuralink},
	urldate = {2019-07-19},
	date = {2019-07-18},
	langid = {english},
	file = {Full Text PDF:C\:\\Users\\fsram\\Zotero\\storage\\NMX6T9ZV\\Musk und Neuralink - 2019 - An integrated brain-machine interface platform wit.pdf:application/pdf;Snapshot:C\:\\Users\\fsram\\Zotero\\storage\\V3SB7PR8\\703801v2.html:text/html}
}

@report{musk_integrated_2019-1,
	title = {An integrated brain-machine interface platform with thousands of channels},
	url = {http://biorxiv.org/lookup/doi/10.1101/703801},
	abstract = {Brain-machine interfaces ({BMIs}) hold promise for the restoration of sensory and motor function and the treatment of neurological disorders, but clinical {BMIs} have not yet been widely adopted, in part because modest channel counts have limited their potential. In this white paper, we describe Neuralink’s first steps toward a scalable high-bandwidth {BMI} system. We have built arrays of small and flexible electrode “threads”, with as many as 3,072 electrodes per array distributed across 96 threads. We have also built a neurosurgical robot capable of inserting six threads (192 electrodes) per minute. Each thread can be individually inserted into the brain with micron precision for avoidance of surface vasculature and targeting specific brain regions. The electrode array is packaged into a small implantable device that contains custom chips for low-power on-board amplification and digitization: the package for 3,072 channels occupies less than (23 × 18.5 × 2) mm3. A single {USB}-C cable provides full-bandwidth data streaming from the device, recording from all channels simultaneously. This system has achieved a spiking yield of up to 85.5 \% in chronically implanted electrodes. Neuralink’s approach to {BMI} has unprecedented packaging density and scalability in a clinically relevant package.},
	institution = {Neuroscience},
	type = {preprint},
	author = {Musk, Elon and {Neuralink}},
	urldate = {2019-07-19},
	date = {2019-07-17},
	langid = {english},
	doi = {10.1101/703801},
	file = {Musk und Neuralink - 2019 - An integrated brain-machine interface platform wit.pdf:C\:\\Users\\fsram\\Zotero\\storage\\QJ8AVP75\\Musk und Neuralink - 2019 - An integrated brain-machine interface platform wit.pdf:application/pdf}
}

@online{noauthor_ai_nodate,
	title = {{AI} Safety Myths},
	url = {https://futureoflife.org/background/aimyths/},
	abstract = {Addressing {AI} safety myths: Why do we need research to ensure {AI} remains safe and beneficial? What are the benefits and risks of artificial intelligence?},
	titleaddon = {Future of Life Institute},
	urldate = {2019-08-06},
	langid = {american},
	file = {Snapshot:C\:\\Users\\fsram\\Zotero\\storage\\G86HX2B3\\aimyths.html:text/html}
}

@online{noauthor_facing_nodate,
	title = {Facing the Intelligence Explosion},
	url = {https://intelligenceexplosion.com/},
	urldate = {2019-08-06},
	file = {Facing the Intelligence Explosion:C\:\\Users\\fsram\\Zotero\\storage\\KSVRN4I7\\intelligenceexplosion.com.html:text/html}
}

@book{armstrong_smarter_2014,
	title = {Smarter Than Us: The Rise of Machine Intelligence},
	shorttitle = {Smarter Than Us},
	abstract = {What happens when machines become smarter than humans? Forget lumbering Terminators. The power of artificial intelligence ({AI}) lies in its potential for superior intelligence, not physical strength or laser guns. Humans steer the future not because we’re the strongest or fastest species, but because of our cognitive advances; and once {AI} systems surpass us on this front, we’ll be handing them the steering wheel.What promises—and perils—does smarter-than-human {AI} present? Can we instruct {AI} systems to steer the future as we desire? What goals should we program into them? Stuart Armstrong’s new book navigates these questions with clarity and wit.Though an understanding of the problem is only beginning to spread, researchers from fields ranging from philosophy to computer science to economics are working together to conceive and test solutions. Are we up to the challenge?A mathematician by training, Armstrong is a Research Fellow at the Future of Humanity Institute ({FHI}) at Oxford University. His research focuses on formal decision theory, the risks and possibilities of {AI}, the long term potential for intelligent life (and the difficulties of predicting this), and anthropic (self-locating) probability.},
	pagetotal = {64},
	publisher = {Machine Intelligence Research Institute},
	author = {Armstrong, Stuart},
	date = {2014-02-01}
}

@book{noauthor_facing_nodate-1,
	title = {‎Facing the Intelligence Explosion},
	url = {https://books.apple.com/us/book/facing-the-intelligence-explosion/id623915471},
	abstract = {‎Computers \& Internet · 2013},
	urldate = {2019-08-06},
	langid = {english},
	file = {Snapshot:C\:\\Users\\fsram\\Zotero\\storage\\DKJU5DVC\\id623915471.html:text/html}
}

@article{kaplan_siri_2019,
	title = {Siri, Siri, in my hand: Who’s the fairest in the land? On the interpretations, illustrations, and implications of artificial intelligence},
	volume = {62},
	issn = {0007-6813},
	url = {http://www.sciencedirect.com/science/article/pii/S0007681318301393},
	doi = {10.1016/j.bushor.2018.08.004},
	shorttitle = {Siri, Siri, in my hand},
	abstract = {Artificial intelligence ({AI})—defined as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation—is a topic in nearly every boardroom and at many dinner tables. Yet, despite this prominence, {AI} is still a surprisingly fuzzy concept and a lot of questions surrounding it are still open. In this article, we analyze how {AI} is different from related concepts, such as the Internet of Things and big data, and suggest that {AI} is not one monolithic term but instead needs to be seen in a more nuanced way. This can either be achieved by looking at {AI} through the lens of evolutionary stages (artificial narrow intelligence, artificial general intelligence, and artificial super intelligence) or by focusing on different types of {AI} systems (analytical {AI}, human-inspired {AI}, and humanized {AI}). Based on this classification, we show the potential and risk of {AI} using a series of case studies regarding universities, corporations, and governments. Finally, we present a framework that helps organizations think about the internal and external implications of {AI}, which we label the Three C Model of Confidence, Change, and Control.},
	pages = {15--25},
	number = {1},
	journaltitle = {Business Horizons},
	shortjournal = {Business Horizons},
	author = {Kaplan, Andreas and Haenlein, Michael},
	urldate = {2019-08-06},
	date = {2019-01-01},
	keywords = {Artificial intelligence, Big data, Deep learning, Expert systems, Internet of Things, Machine learning},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\fsram\\Zotero\\storage\\UHADMB2F\\Kaplan und Haenlein - 2019 - Siri, Siri, in my hand Who’s the fairest in the l.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\fsram\\Zotero\\storage\\U8L65GNB\\S0007681318301393.html:text/html}
}

@article{yudkowsky_intelligence_nodate,
	title = {Intelligence Explosion Microeconomics},
	abstract = {I. J. Good’s thesis of the “intelligence explosion” states that a suﬃciently advanced machine intelligence could build a smarter version of itself, which could in turn build an even smarter version, and that this process could continue to the point of vastly exceeding human intelligence. As Sandberg (2010) correctly notes, there have been several attempts to lay down return on investment formulas intended to represent sharp speedups in economic or technological growth, but very little attempt has been made to deal formally with Good’s intelligence explosion thesis as such.},
	pages = {96},
	author = {Yudkowsky, Eliezer},
	langid = {english},
	file = {Yudkowsky - Intelligence Explosion Microeconomics.pdf:C\:\\Users\\fsram\\Zotero\\storage\\X8TD54GW\\Yudkowsky - Intelligence Explosion Microeconomics.pdf:application/pdf}
}

@book{bostrom_superintelligence:_2014,
	location = {Oxford},
	title = {Superintelligence: Paths, Dangers, Strategies},
	isbn = {978-0-19-967811-2},
	shorttitle = {Superintelligence},
	abstract = {A New York Times {bestsellerSuperintelligence} asks the questions: What happens when machines surpass humans in general intelligence? Will artificial agents save or destroy us? Nick Bostrom lays the foundation for understanding the future of humanity and intelligent life. The human brain has some capabilities that the brains of other animals lack. It is to these distinctive capabilities that our species owes its dominant position. If machine brains surpassed human brains in general intelligence, then this new superintelligence could become extremely powerful - possibly beyond our control. As the fate of the gorillas now depends more on humans than on the species itself, so would the fate of humankind depend on the actions of the machine superintelligence.But we have one advantage: we get to make the first move. Will it be possible to construct a seed Artificial Intelligence, to engineer initial conditions so as to make an intelligence explosion survivable? How could one achieve a controlled detonation?This profoundly ambitious and original book breaks down a vast track of difficult intellectual terrain. After an utterly engrossing journey that takes us to the frontiers of thinking about the human condition and the future of intelligent life, we find in Nick Bostrom's work nothing less than a reconceptualization of the essential task of our time.},
	pagetotal = {328},
	publisher = {Oxford University Press},
	author = {Bostrom, Nick},
	date = {2014-07-03}
}

@online{noauthor_eliezer_nodate,
	title = {Eliezer Yudkowsky on Intelligence Explosion - {YouTube}},
	url = {https://www.youtube.com/watch?v=D6peN9LiTWA},
	urldate = {2019-08-07},
	file = {Eliezer Yudkowsky on Intelligence Explosion - YouTube:C\:\\Users\\fsram\\Zotero\\storage\\EBHJDYBF\\watch.html:text/html}
}

@article{easterlin_worldwide_2000,
	title = {The Worldwide Standard of Living since 1800},
	volume = {14},
	issn = {0895-3309},
	url = {https://www.jstor.org/stable/2647048},
	pages = {7--26},
	number = {1},
	journaltitle = {The Journal of Economic Perspectives},
	author = {Easterlin, Richard A.},
	urldate = {2019-08-09},
	date = {2000}
}

@book{goertzel_advances_2007,
	title = {Advances in Artificial General Intelligence: Concepts, Architectures and Algorithms : Proceedings of the {AGI} Workshop 2006},
	isbn = {978-1-58603-758-1},
	shorttitle = {Advances in Artificial General Intelligence},
	abstract = {"The topic of this book the creation of software programs displaying broad, deep, human-style general intelligence is a grand and ambitious one. And yet it is far from a frivolous one: what the papers in this publication illustrate is that it is a fit and proper subject for serious science and engineering exploration. No one has yet created a software program with human-style or (even roughly) human-level general intelligence but we now have a sufficiently rich intellectual toolkit that it is possible to think about such a possibility in detail, and make serious attempts at design, analysis and engineering. possibility in detail, and make serious attempts at design, analysis and engineering. This is the situation that led to the organization of the 2006 {AGIRI} (Artificial General Intelligence Research Institute) workshop; and to the decision to publish a book from contributions by the speakers at the conference.The material presented here only scratches the surface of the {AGI}-related R\&D work that is occurring around the world at this moment. But the editors are pleased to have had the chance to be involved in organizing and presenting at least a small percentage of the contemporary progress."},
	pagetotal = {305},
	publisher = {{IOS} Press},
	author = {Goertzel, Ben and Wang, Pei},
	date = {2007},
	langid = {english},
	note = {Google-Books-{ID}: t2G5srpFRhEC},
	keywords = {Computers / Intelligence ({AI}) \& Semantics}
}

@incollection{muller_future_2016,
	location = {Cham},
	title = {Future Progress in Artificial Intelligence: A Survey of Expert Opinion},
	isbn = {978-3-319-26485-1},
	url = {https://doi.org/10.1007/978-3-319-26485-1_33},
	series = {Synthese Library},
	shorttitle = {Future Progress in Artificial Intelligence},
	abstract = {There is, in some quarters, concern about high–level machine intelligence and superintelligent {AI} coming up in a few decades, bringing with it significant risks for humanity. In other quarters, these issues are ignored or considered science fiction. We wanted to clarify what the distribution of opinions actually is, what probability the best experts currently assign to high–level machine intelligence coming up within a particular time–frame, which risks they see with that development, and how fast they see these developing. We thus designed a brief questionnaire and distributed it to four groups of experts in 2012/2013. The median estimate of respondents was for a one in two chance that high-level machine intelligence will be developed around 2040–2050, rising to a nine in ten chance by 2075. Experts expect that systems will move on to superintelligence in less than 30 years thereafter. They estimate the chance is about one in three that this development turns out to be ‘bad’ or ‘extremely bad’ for humanity.},
	pages = {555--572},
	booktitle = {Fundamental Issues of Artificial Intelligence},
	publisher = {Springer International Publishing},
	author = {Müller, Vincent C. and Bostrom, Nick},
	editor = {Müller, Vincent C.},
	urldate = {2019-09-05},
	date = {2016},
	langid = {english},
	doi = {10.1007/978-3-319-26485-1_33},
	keywords = {Artificial intelligence, {AI}, Expert opinion, Future of {AI}, Humanity, Intelligence explosion, Machine intelligence, Opinion poll, Progress, Singularity, Superintelligence}
}

@article{vinge_coming_nodate,
	title = {The Coming Technological Singularity: How to Survive in the Post-Human Era},
	url = {https://edoras.sdsu.edu/~vinge/misc/singularity.html},
	abstract = {Within thirty years, we will have the technological
              means to create superhuman intelligence. Shortly after,
              the human era will be ended.

                   Is such progress avoidable? If not to be avoided, can
              events be guided so that we may survive?  These questions
              are investigated. Some possible answers (and some further
              dangers) are presented.},
	author = {Vinge, Vernor},
	urldate = {2019-09-07},
	langid = {english},
	file = {The Coming Technological Singularity:C\:\\Users\\fsram\\Zotero\\storage\\WS8XK5AD\\singularity.html:text/html}
}

@online{yudkowsky_what_2001,
	title = {What is Friendly {AI}? {\textbar} Kurzweil},
	url = {https://www.kurzweilai.net/what-is-friendly-ai},
	abstract = {“What is Friendly {AI}?” is a short introductory article to the theory of “Friendly {AI},” which attempts to answer questions such as those above. Further material on Friendly {AI} can be found at the Singularity Institute website at http://singinst.org/friendly/whatis.html, including a book-length explanation.},
	titleaddon = {What is Friendly {AI}?},
	author = {Yudkowsky, Eliezer},
	urldate = {2019-10-01},
	date = {2001-05-03},
	file = {What is Friendly AI? | Kurzweil:C\:\\Users\\fsram\\Zotero\\storage\\P8I4KGHB\\what-is-friendly-ai.html:text/html}
}