% !TeX root = document.tex
\chapter{Probleme einer allgemeinen künstlichen Intelligenz}
\section{Fehlerhafte Vorstellungen einer KI-Katastrophe}
In der allgemeinen Bevölkerung überwiegen fehlerhafte Vorstellungen einer KI-Katastrophe. Die folgenden Unterkapitel dienen der Aufklärung von Missverständnissen und Mythen.
\subsection{KI, die ein Bewusstsein erlangt}
In der Laienwelt sowie in großen Teilen der KI-Forschung ist eine These bekannt, die besagt, dass eine KI ab einer bestimmten Intelligenzschwelle ein Bewusstsein erlangt. Anders als vielerorts angenommen hätte selbst ein Beweis dieser These keinerlei Auswirkungen auf die AKI-Forschung. Diese beschäftigt sich ausschließlich mit der Entwicklung und den Folgen einer AKI. Ein Szenario, in dem ein autonomes Fahrzeug eine Person X \emph{bewusst} vom Ort A zum Ort B chauffiert, wird zum gleichen Ergebnis führen wie ein Szenario, in dem selbiges \emph{unbewusst} geschieht. Somit ist der \emph{Bewusstseinszustand} einer AKI zwar noch nicht wissenschaftlich erforscht -- damit beschäftigt sich ein eigenes Teilgebiet der KI-Forschung -- , zum Erreichen einer sicheren KI ist er aber irrelevant. \vgl{noauthor_ai_nodate-1}
\subsection{Roboter als Auslöser einer Katastrophe}
Ein in der Populärliteratur besonders stark ausgeprägter Mythos ist jener einer existenziellen Bedrohung durch Roboter, die die Welt erobern. Geschuldet ist dies nicht nur den klassischen Science-Fiction-Romanen. Es ist eine domänenübergreifend anzutreffende Neigung der Spezies Mensch, Wesen oder Systeme, die einem unverständlich sind, zu vermenschlichen. Von den Wikingern, nach denen ein menschenähnliches Wesen namens Thor Donner und Blitz lenkt, zu den modernen Weltreligionen, in denen Anthropomorphismus in selbigem Ausmaß gang und gäbe ist, ist dieses Phänomen schon seit jeher in der Geschichte des Menschen zu beobachten. Ich erkäre mir den Anthropomorphismus als einen misslungenen Erklärungsversuch unseres Gehirns für unverständliche Beobachtungen.

Die größte Sorge der Forschung nach einer sicheren AKI gilt nicht möglichen Robotern, sondern der Intelligenz selbst, genauer gesagt einer Intelligenz, deren Ziele nicht eindeutig mit den unseren übereinstimmen. Intelligenz ermöglicht Kontrolle, und eine fortgeschrittene Intelligenz braucht auch keine Roboter, um ihre Ziele zu erreichen. Heutzutage reicht eine Internetverbindung völlig aus. \vgl{noauthor_ai_nodate-1}
\subsection{Bösartige AKI} \label{bösartigeKI}
Eine AKI, deren Ziele nicht eindeutig mit den unseren übereinstimmen, ist nicht die Folge ihres \emph{bösartigen} Willens, sondern die Folge einer unzureichend spezifizierten Zielsetzung. Ein autonomes Fahrzeug, dessen alleiniges Ziel es ist, seine Insassen vom Ort A zum Ort B zu befördern, wird nicht auf die Gesundheit anderer Verkehrsteilnehmer achten, die Straßenverkehrsordnung nicht befolgen, nicht nur auf Straßen fahren, unangenehm Bremsen, unökologisch Beschleunigen und nicht nach den weiteren unzähligen, geschriebenen und ungeschriebenen menschlichen Werten und Normen handeln.

Es gibt keinen \emph{Geist in der Maschine}, der unser geschriebenes Programm durchliest und uns auf alle Stellen aufmerksam macht, die wir nicht so gemeint haben, wie wir sie geschrieben haben. Eine AKI ist nicht \emph{gut} oder \emph{böse}, sie folgt nur unseren Anweisungen. \vgl[1]{yudkowsky_complex_2011}

\section{Auswirkungen einer AKI}
\subsection{Arbeitslosigkeit durch Automatisierung}
Seit der industriellen Revolution werden immer mehr Arbeitsstellen automatisiert und durch Maschinen ersetzt, die in der Regel schneller und genauer arbeiten und meist auch kosteneffizienter sind. Es gibt also wirtschaftliche Anreize zur Automatisierung. Dieses Phänomen, das heute schon bei schwacher KI beobachtet werden kann, wird in Zukunft bei einer fortgeschrittenen und letzten Endes allgemeinen künstlichen Intelligenz verstärkt auftreten. Die -- zumindest temporäre -- Arbeitslosigkeit für den größten Teil der Bevölkerung ist zu erwarten. \vgl[3-4]{sotala_responses_2015}

\citeauthor{okeefe_windfall_2020} schlagen als Gegenmaßnahme eine sogenannte \emph{Windfall-Klausel} vor. Unternehmen, die sich dieser Klausel verpflichten, müssen im Falle eines großen Profitsprungs, der durch eine KI verursacht wurde, einen gewissen Betrag für gemeinnützige Zwecke spenden. Im Falle einer Massenarbeitslosigkeit kann ein solcher Geldtopf dann für das Umtrainieren oder Unterstützen der Arbeitskräfte verwendet werden. Auch ein bedingungsloses Grundeinkommen wäre unter Umständen umsetzbar. \vgl[2]{okeefe_windfall_2020}

%https://80000hours.org/podcast/episodes/allan-dafoe-politics-of-ai/
%https://80000hours.org/topic/priority-paths/ai-policy/
\subsection{Machtverschiebung -und konzentration}
\enquote{Whoever leads in AI will rule the world} ist ein Zitat des russisches Staatspräsidenten Vladimir Putin. Es verdeutlicht die weltpolitische Wichtigkeit von KI und dessen Entwicklung in Richtung einer AKI.

Fortschritte in der Entwicklung autonomer Waffensystem könnten zu einer Verschiebung der militärischen Macht von Ländern und Gruppierungen führen und so die bestehenden Mächtegleichgewichte gefährden.

Wenn eine Institution einen Vorsprung in der Entwicklung ihrer KI erlangt, so bringt das auch verstärkte politische Macht mit sich. Jüngst konnte man dessen potentiell verheerende Folgen an dem Beispiel von \emph{Camebridge Analytica} beobachten. \vgl[14-15]{duettmann_artificial_2017}

\citeauthor{cihon_should_2019} schreiben in einem Bericht über die Umsetzbarkeit und Vorteilhaftigkeit einer möglichen zentralisierten KI-Institution. Sie kommen zum Schluss, dass eine ausreichend durchdacht konzipierte Institution einen positiven Effekt auf die Entwicklung einer angepassten AKI haben könnte. In naher Zukunft scheint eine solche Organisation aufgrund des bestehenden Mächteungleichgewichts unwahrscheinlich. \vgl[6-9]{cihon_should_2019}

\subsection{Missbrauch durch Cyberattacken}
Heutige Cyberattacken haben meist zur Folge, dass Geld oder Daten gestohlen werden. Im schlimmsten Falle können sie auch zu Menschentoden führen, beispielsweise bei Angriffen auf kritische Infrastruktur wie Krankenhäuser. Die Folgen bei einer Attacke auf eine mögliche AKI wären schlimmer: Ein einziger Angriff könnte ein Existenzrisiko für die Menschheit darstellen. Das Problem der AKI-Cybersicherheit wird zukünftigen Arbeiten überlassen und in dieser Arbeit nicht weiter erläutert. \vgl[8]{yampolskiy_artificial_2016}
\subsection{Unangepasste AKI}
Ein noch viel größeres Risiko ist eine ungangepasste AKI, weil diese zu selbigen Folgen führen würde wie ein Missbrauch durch Cyberattacken und dazu keinen böswilligen Akteur braucht.\vgl[14]{yudkowsky_intelligence_2013} \emph{Unangepasst} ist eine AKI, wenn sie nicht auf die Werte der Menschheit ausgerichtet ist und deshalb die ihr vorgegebenen Ziele nicht in der Art und Weise umsetzt, wie das von ihrem Operator gewollt war. Das folgende Kapitel beschäftigt sich mit Ansätzen, eine AKI \emph{anzupassen}.
%\section{KI-Ethik}
%Damit eine AKI ihre Ziele auf menschengewollte Art und Weise erfüllt, muss sie menschliche Werte teilen. Das folgende Kapitel behandelt Probleme bei einer etwaigen Werteformulierung.
%\subsection{Gesamtmenschheitlicher Konsens über gemeinsame Werte}
%
%Da eine mögliche AKI mit großer Wahrhscheinlichkeit globale Auswirkungen hätte, ist ein Miteinbeziehen der gesamten Meschheit unabdingbar.
%
%KOMMENTAR: Reflective Equilibrium; Ideal advisor Theory; EU-Richtlinien
%\subsection{\quotes{Gute} und \quotes{schlechte} menschliche Werte}
%KOMMENTAR:
%
%Selbst Forscher, die ein komsopolitisches Weltbild haben, das deutlich anders ist, als jenes der heutigen Gesellschaft, können das Nichteinbinden von menschlichen Werten nicht befürworten, denn dies würde schlimmere Folgen mit sich ziehen, als das Einbinden menschlicher Werte und die daraus resultierenden Negativfolgen in Form von systematischer Diskriminierung.
%\section{Demokratisierung einer AKI}
%\section{Maschinelle Werteanpassung}

%\section{Verzerrungen}
%\subsection{Verzerrung in der Risikoeinschätzung}
%KOMMENTAR: Auch Zeitpunkt einer AKI
%\subsection{Verzerrung in der Werteformulierung}
%\subsection{Verzerrung in der Kodierung}
%Nutzenfunktion (eng. \emph{utility function})

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "document"
%%% End:
