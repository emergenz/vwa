 % !TeX root = document.tex
\chapter{Allgemeine künstliche Intelligenz}
\section{Definition von Intelligenz}
Seit Jahrhunderten versuchen Wissenschaftler und Laien gleichermaßen eine Definition für den Intelligenzbegriff zu finden. Da bis heute keine Defintion ihre Vollständig- oder Richtigkeit beweisen konnte, wird in dieser Arbeit der Einfachheit halber versucht, den Begriff durch Beobachtungen zu erklären, wie Eliezer Yudkowsky in dem Podcast \quotes{AI: Racing Toward the Brink} vorschlägt.
\begin{enumerate}
\item Menschen waren auf dem Mond.
\item Mäuse waren nicht auf dem Mond.
\end{enumerate}
Yudkowsky wählt dieses Beispiel, um zwei Thesen zu belegen:

Menschen sind \emph{intelligenter} als Mäuse, weil sie \emph{domänenübergreifend} arbeiten können. Damit sei das \emph{domänenübergreifende} Erlernen neuer Fähigkeiten ein zentraler Teil des Intelligenzbegriffs.


Die natürliche Selektion ist neben der menschlichen Lernfähigkeit eines der wenigen Vorgänge, die zu einer \emph{domänenübergreifenden} Leistungsoptimierung führt, das oben genannte Beispiel belegt jedoch, dass die Menschheit auch Orte erreichen kann, wofür die natürliche Selektion sie nicht vorbereitet hat. Dies und die Tatsache, dass die Evolution Millionen Jahre benötigte, um aus dem Homo sapien den Homo erectus zu formen \vgl[508]{grzimek_grzimeks_1979}, während die Menschheit sich in wenigen Jahrhunderten logarithmisch optimiert hat, zeigt, dass der Mensch der schnellere und effizientere Optimierer ist.\emph{Effizienz} ist also ein weiterer Teilaspekt der Intelligenz.\vgl[9]{yudkowsky_intelligence_2013}

\section{Definition von künstlicher Intelligenz}
\zit[15]{kaplan_siri_2019}{Artificial intelligence (AI)–—defined as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation}

Laut angeführter Definiton muss eine künstliche Intelligenz nicht nur Daten richtig interpretieren, sondern auch die dadurch gewonnen Erkenntnisse mittels \emph{dynamischer Anpassung} zur Erreichung bestimmter Ziele benützen können.

Diese Definition enthält die Idee des \emph{domänenübergreifenden} Lernens im Gegensatz zum oben beschriebenen Ansatz zur Intelligenzerklärung nicht, was laut Experten jedoch nicht an einer unvollständigen Definition liegt, sondern vielmehr daran, dass wir den Begriff der KI in einer Art gebrauchen, wofür er nicht vorgesehen war. Um Missverständnisse zu vermeiden, wird für KI wie sie heutzutage bereits in Benutzung ist der Begriff schwache KI (engl. \emph{weak AI} oder \emph{narrow AI}) verwendet. \vgl[18--19]{bostrom_superintelligence:_2014} Dieser beschreibt eine \emph{domänenspezifische} KI.

\section{Definition von allgemeiner künstlicher Intelligenz}
Als allgemeine künstliche Intelligenz (AKI; auch \emph{starke KI} genannt; engl. \emph{strong AI} oder \emph{general AI}) bezeichnet man ein technisch fortgeschrittenes System, dessen Lernkapazität nicht auf einzelne Domänen begrenzt ist, sondern als \emph{allgemein} bezeichnet werden kann. \vgl[1]{goertzel_advances_2007}

\section{Werte einer allgemeinen künstlichen Intelligenz}
Der \emph{Instrumental Convergence Thesis} nach gibt es bestimmte Ressourcen, die für eine AKI beim Erreichen der ihnen vorgegebenen Ziele in den meisten Fällen behilflich sind. Dazu gehören unter anderem Materie oder Energie, eine AKI wird jedoch auch Quellcodeveränderungen, die zu einem potenziellen Erschweren ihrer Zielerfüllung führen könnten, zu stoppen versuchen. Sie kann also Menschen schaden, ohne dass sie Werte besitzt, die dies explizit fordern. Für ein rein rational denkendes System sind Menschen nichts als eine Ansammlung von Atomen, die auch für das Erreichen seiner Ziele eingesetzt werden können.\vgl[14]{yudkowsky_intelligence_2013}

Ein fortgeschrittenes System wie eine AKI muss ihre Ziele auf der Basis von Werten verfolgen, von denen die Menschheit als Gesamtes profitiert, um ungewollten Nebenwirkungen wie der in der Einleitung genannten Auslöschung der Menschheit durch unpräzises Definieren ihrer Ziele mit größtmöglicher Sicherheit vorzubeugen. Aber auch Missbrauch in Form einer Machtkonzentration oder Ähnlichem muss unter allen Umständen vermieden werden.

Der Ansatz eine \emph{antropomorphe} Maschine, also ein System mit menschenähnlichen Eigenschaften, zu entwickeln, gilt deshalb als veraltet. Während einige menschliche Werte und Eigenschaften implementiert werden müssen, um mögliche Dissonanzen zwischen der AKI und der Menschheit zu vermeiden, dürfen andere menschliche Eigenschaften nicht übernommen werden. Ansonsten werden Vorurteile ohne rationalem Grundsatz in das System aufgenommen, was dazu führt, dass eine AKI beim Erreichen ihrer Ziele Frauen oder Afrikaner benachteiligt oder Asiaten automatisch als intelligenter einstuft.\vgl{yudkowsky_what_2001}

Menschliche Werte in einer Programmiersprache nachzubilden ist nach der \emph{Complexity of Value Thesis} aufwendig, da sie - selbst in idealisierter Form - eine hohe algorithmische Komplexität vorweisen. Daher muss eine AKI komplexe Informationen gespeichert haben, damit sie die ihr vorgegebenen Ziele auf eine menschengewollte Weise erfüllen kann. Dabei reichen auch keine vereinfachten Zielstellungen wie \quotes{Menschen glücklich machen}.\vgl[13-14]{yudkowsky_intelligence_2013} Es gibt keinen \quotes{Geist im System}, der diese abstrakte Zielsetzung ohne Weiteres versteht.

Hibbard beschreibt in seinem Buch eine Möglichkeit, Maschinen das abstrakte Gefühl der Freude zu erklären. Dabei lernt eine hypothetische KI durch einen riesigen Datensatz, bei welchen Gesichtsausdrücken, Stimmeigenschaften und Körperhaltungen ein Mensch glücklich ist.\vgl[115]{hibbard_super-intelligent_2002} Yudkowsky ist der Meinung, dass dies keinesfalls eine Lösung für das Problem der exakten Zielsetzung ist und führt Hibbards Gedankenexperiment fort. Falls diese KI nun ein Bild von einem winzigen, molekularen Smiley-Gesicht sieht, so ist es nicht unwahrscheinlich, dass die KI dies als Glücklichsein interpretiert und das Universum in eine einzige Ansammlung von winzigen, molekularen Smiley-Gesichtern umzuwandeln versucht, um den höchstmöglichen Zustand des Glücklichseins zu erreichen. \vgl[3]{yudkowsky_complex_2011}

\section{Wann wird es sie geben?}
Eine Befragung durch die KI-Wissenschaftler V. C. Müller und N. Bostrom kam zu dem Ergebnis, dass KI-Experten dem Erreichen einer AKI in den Jahren 2040 bis 2050 eine Wahrscheinlichkeit von über 50, und dem Erreichen bis 2075 eine Wahrscheinlichkeit von 90 Prozent zuordnen. \vgl[566]{muller_future_2016} Es ist also - sollten sich die Expertenmeinungen als richtig herausstellen - davon auszugehen, dass eine AKI bereits in diesem Jahrhundert zur Realität und bereits für die jetzige Generation mehr als nur relevant sein wird. 

\section{Die These der Intelligenzexplosion}
Eine AKI wird - unabhängig von ihren Zielen - Selbstoptimierung hinsichtlich ihrer Intelligenz anstreben, weil sie dadurch ihre Ziele schneller und effizienter erreichen kann. Sobald die erste KI programmiert werden würde, die qualitativ bessere - also noch intelligentere - KIs programmieren könnte, käme es zu einem Kreislauf der kognitiven Leistungssteigerung. Die KI der Tochtergeneration könnte nun als verbesserter KI-Designer noch bessere KIs programmieren. Anders als bei biologischer Intelligenz kann eine KI bei Verfügbarkeit entsprechender Hardware einfach kopiert werden. Eine Gruppe von KIs hätte dann gemeinsam quantitativ und qualitativ höhere kognitive Fähigkeiten, ähnlich einer Schwarmintelligenz. Dieser hypothetische Kreislauf ist die Grundlage der These der Intelligenzexplosion. Nach ihr wird ab einer bestimmten Schwelle die Leistungssteigerung mit jeder KI-Iteration exponentiell größer, was zu einer \emph{Superintelligenz} führt, die der Menschheit kognitiv um einige Größenordnungen überlegen ist. \vgl[13]{muehlhauser_intelligence_2012}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "document"
%%% End:
