% !TeX root = document.tex
\chapter{Maschinelle Werteanpassung}

Es ist schwer, menschliche Werte in Computersysteme zu programmieren (siehe Kapitel \ref{Werte}), deshalb haben \citeauthor{irving_ai_2018} einen anderen Ansatz der Werteanpassung verfolgt: die des menschlichen Feedbacks durch \emph{Deep reinforcement learning}. Das folgende Unterkapitel dient der Erklärung von wichtigen Lernverfahren der KI-Forschung, um die wissenschaftlichen Arbeiten von \citeauthor{irving_ai_2018} zu verstehen.

\section{KI-Lernverfahren}
\subsection{Reinforcement Learning}
Reinforcement Learning (RL, dt. \emph{bestärkendes Lernen}) beschreibt ein Lernverfahren einer KI, bei der sie durch durch Erfolg und Misserfolg, durch Belohnung und Bestrafung lernt. \citeauthor{russell_artificial_2016} erklären RL zusammengefasst so: \zit[831]{russell_artificial_2016}{Imagine playing a new game whose rules you don't know; after a hundred or so moves, your opponent announces, \enquote{You lose.} This is reinforcement learning in a nutshell.}

Die Aufgabe von RL ist es, wahrgenommene Belohnungen und Bestrafungen zu benutzen, um die optimale Verfahrensweise (eng. \emph{policy}) in einer gegebenen Umgebung zu finden. Dabei hat die KI a priori kein Wissen über ihre Umgebung oder Nutzfunktion. Die Nutzfunktion, definiert über Umgebungszustände, zeigt dabei den Nutzen einer bestimmten Verfahrensweise. Die optimale Verfahrensweise ist diejenige, die den höchsten erwarteten Nutzen bringt.

RL wird in Bereichen eingesetzt, in denen es nicht genug Daten gibt, oder in denen es nicht lohnenswert ist, die notwendige Menge an Daten zu verarbeiten, um eine KI auf alle möglichen Umgebungszustände vorzubereiten. Eine KI, die beispielsweise versucht, Schach zu lernen, müsste $10^{120}$ (auch Shannon-Zahl genannt) verschiedene Schachspiele gesehen haben, um allein anhand von Beispielen auf jede Situation vorbereitet zu sein. \vgl[4]{shannon_programming_1988} Bei RL vermittelt man der KI stattdessen, wann sie gewonnen oder verloren hat. Sie sucht dann auf Basis dieser Informationen eine Funktion, die die Gewinnwahrscheinlichkeit jeder gegebenen Position einigermaßen akkurat einschätzt.
\vgl[830-831]{russell_artificial_2016}

\subsection{Deep Learning}
Deep Learning (DL, dt. \emph{mehrschichtiges Lernen}) ist ein Teilbereich des maschinellen Lernens. Dabei versucht eine KI Inputdaten mit Hilfe von Hierarchien von Konzepten zu verstehen. Der Grundansatz von DL ist das Verstehen von komplexen Konzepten durch Kombinieren von einfacheren Konzepten (siehe Abbildung \ref{deeplearningimg}). Diese Konzeptschichten werden in DL fast immer mit Hilfe von künstlichen neuronalen Netzen (KNN, engl. \emph{artificial neural network, ANN}) gelernt. \vgl[8]{chollet_deep_2017} Die Anzahl der Schichten wird auch Tiefe (eng. \emph{depth}) genannt, daher der Name Deep Learning.
\vgl[1-8]{Goodfellow-et-al-2016}

DL wird heute vor allem in den Bereichen der Sprach- und Bilderkennung sowie der maschinellen Übersetzung eingesetzt. \vgl[25-26]{Goodfellow-et-al-2016}

\begin{figure}
  \includegraphics[width=\textwidth]{deeplearning}
  \caption{Veranschaulichung eines DL-Modells. Die KI bekommt rohe Pixeldaten als Input. Mit jeder Schicht wendet sie ein neues Konzept auf das vorherige an, die Konzepte sind also aufbauend. Durch Analyse der Helligkeit umgebener Pixel werden Ränder erkannt (1. Schicht). Ansammlungen von Rändern werden als Ecken und Konturen identifiziert (2. Schicht). Durch zusammenhängende Ecken und Konturen können ganze Objektteile bestimmt werden (3. Schicht).  \bildquelle[6]{Goodfellow-et-al-2016}}
  \label{deeplearningimg}
\end{figure}
\subsection{Deep Reinforcement Learning}

Deep Reinforcement Learning (DRL, dt. \emph{mehrschichtiges bestärkendes Lernen}) kombiniert die Ansätze von RL mit denen von DL. Neuronale Netze werden trainiert, um jeder möglichen Aktion in einer gegebenen Umgebungsposition einen Nutzwert zuzuteilen. Ihr Ziel ist es, die nützlichste Aktion zu finden. \vgl{nicholson_beginners_nodate} Auf der Abbildung \ref{deepreinforcementlearningimg} wird dieser Vorgang mit einem Frame des Spiels \emph{Mario Bros.} als Input veranschaulicht. Diese Nutzwertzuteilung ermöglicht eine signifikante Leistungsteigerung von RL in bestimmten Domänen.

\citeauthor{mnih_human-level_2015} haben einen Algorithmus entwickelt, mit dem eine KI allein anhand von Pixeln als Input gelernt hat, 49 verschiedene \emph{Atari 2600}-Spiele zu spielen, 29 davon sogar auf menschenähnlichem Niveau. \vgl{mnih_human-level_2015}



\begin{figure}
  \includegraphics[width=\textwidth]{deepreinforcementlearning}
  \caption{Die Umgebung ist das Level, in dem sich Mario (links unten zu sehen) befindet, die möglichen Aktionen sind: springen, nach links laufen, nach rechts laufen. Die neuronalen Netze teilen jeder Aktion einen Nutzwert zu. Beispiel: springen (5), nach rechts laufen (7), nach links laufen (0). \bildquelle{nicholson_beginners_nodate}}
  \label{deepreinforcementlearningimg}
\end{figure}

\subsection{Inverse Reinforcement Learning}
Inverse Reinforcement Learning (IRL, dt. \emph{umgekehrtes bestärkendes Lernen}) ist ein Lernverfahren, bei dem eine KI versucht, anhand von Input-Output-Paaren die richtige Lösungsfunktion herzuleiten. Dies ist in allen Bereichen sinnvoll, in denen man (noch) nicht weiß, was das Ziel ist oder in denen es schwer ist, das gewollte Verhalten formell in eine Nutzfunktion auszuschreiben. Ein solcher Fall ist das autonome Fahren. Ein angenehmer und sicherer Fahrstil hängt abgesehen von den Verkehrsregeln noch mit vielen anderen Faktoren zusammen: der Sicherheitsabstand, der Bremsstil, die ökonomische Fahrweise, das Spurhalten, das Rechtsfahren, der Abstand vom Randstein, eine angemessene Fahrgeschwindigkeit oder die Anzahl an Spurwechseln um einige zu nennen. Alle relevanten Faktoren müssten formell ausgeschrieben und gewichtet werden, damit das System weiß, dass der Abstand zu Fußgängern beispielsweise wichtiger ist als der Abstand zum Randstein. Nur so kann ein autonomes Fahrzeug im Zweifelsfall die richtigen Entscheidungen treffen. Statt alle relevanten Faktoren auszuformulieren und zu gewichten, zeigt man einer KI Beispiele von angenehmen und sicheren Fahrstilen und lässt die KI die Nutz- und die Lösungsfunktion herleiten und anpassen. \vgl{abbeel_apprenticeship_2004} Nachdem eine Lösungsfunktion gefunden wurde, kann diese durch RL trainiert werden. \vgl[1]{christiano_deep_2017}


\section{Deep Reinforcement Learning von menschlichen Werten}
Die größte Sorge der KI-Forschung ist, dass wir Zielfunktionen unzureichend definieren und eine KI dadurch Schaden anrichtet. Mit anderen Worten: dass eine KI nicht das tut, was wir \enquote{meinen} (siehe Kapitel \ref{bösartigeKI}).\vgl[1]{yudkowsky_complex_2011} IRL löst dieses Problem, da die Zielfunktion von der KI selbst definiert wird. Der Ansatz funktioniert aber nur bei Aufgaben, für die es auch Lösungsdemonstrationen gibt. Eine Alternative ist, das Verhalten des Systems zu gegebenen Zeitpunkten von Menschen beurteilen zu lassen. \citeauthor{christiano_deep_2017} haben eine KI im ersten Schritt ihre Nutzfunktion durch menschliches Feedback lernen lassen. Im zweiten Schritt optimiert die KI ihre Nutzfunktion, sie versucht sich also so zu verhalten, dass der menschliche Begutachter möglichst zufriedengestellt ist. So handelt die KI nach den menschlichen Werten und ihre Ziele stimmen mit den unseren überein. Diese beiden Schritte werden so lange wiederholt, bis die KI das gewünschte Verhalten zeigt (siehe Abbildung \ref{humanfeedbackimg}). \vgl[1-2]{christiano_deep_2017} Es folgt eine formelle Ausformulierung.

Zu jedem Zeitpunkt $t$ empfängt die KI eine Umgebungsobservation $o_t \in \mathcal{O}$ und sendet dann eine Aktion $a_t \in \mathcal{A}$ an die Umgebung. Wir nehmen an, dass ein menschlicher Begutachter seine Präferenz zwischen Trajektoriensegmenten auswählt, wobei ein Trajektoriensegment eine Abfolge von Observationen und Aktionen ist: $\sigma = ((o_0,a_0),(o_1,a_1),...,(o_{k-1},a_{k-1})) \in (\mathcal{O} \times \mathcal{A})^k$. Man schreibt $\sigma^1 \succ \sigma^2$, um auszudrücken, dass der Begutachter das Trajektoriensegment $\sigma^1$ über dem Segment $\sigma^2$ bevorzugt. \vgl[3-4]{christiano_deep_2017}

In den Experimenten von \citeauthor{christiano_deep_2017} bekommt der menschliche Begutachter Trajektoriensegmente in Form von ein- bis zweisekündigen Videoclips zugespielt. Die Begutachtung kommt in eine Datenbank $\mathcal{D}$ bestehend aus dreidimensionalen Arrays ($\sigma^1,\sigma^2,\mu$), wobei $\mu$ eine Distribution über $\{1,2\}$ ist.

\begin{enumerate}
\item Falls eines der Segmente bevorzugt wird, dann wird die jeweilige Auswahl mehr gewichtet.
\item Falls der Begutachter beide als gleich wünschenswert erachtet, so ist $\mu$ eine Konstante.
\item Falls die Segmente als nicht vergleichbar eingestuft werden, dann wird der jeweilige Vergleich aus der Datenbank $\mathcal{D}$ exkludiert.
\vgl[5]{christiano_deep_2017}
\end{enumerate}
  
Weiters stellen \citeauthor{christiano_deep_2017} eine Formel zur Berechnung der Wahrscheinlichkeit $\hat{P}$ auf, dass ein Begutachter das Trajektoriensegment $\sigma^1$ bevorzugt.
\begin{equation}
  \hat{P}[\sigma^1 \succ \sigma^2] = \frac{exp \sum \hat{r} (o_t^1,a_t^1)}{exp \sum \hat{r}(o^1_t,a^1_t) + exp \sum \hat{r}(o^2_t, a^2_t)}
\end{equation}

$\hat{r}$ ist eine Belohnungsfunktion, also eine Funktion, die die Wahrscheinlichkeit angibt, dass die Trajektorie $(o^1,a^1)$ zum Zeitpunkt $t$ zu einer Belohnung führt. Die Summe der Belohnungsfunktionen zu allen Zeitpunkten $t$ ergibt die gesamte erwartete Belohnung für das Trajektoriensegment $\sigma^1$. Der Quotient von der Gesamtbelohnung von $\sigma^1$ und der Summe der Gesamtbelohnungen beider Segmente ergibt $\hat{P}$. Man bemerke, dass die Autoren alle Summen der Gleichung exponieren. Das liegt daran, dass die Belohnungswahrscheinlichkeit mit zunehmender Zeit exponentiell steigt. Genauso wie der Elopunkten-Unterschied zwischen verschiedenen Schachspielern in etwa die Wahrscheinlichkeit angibt, dass einer gegen den anderen gewinnt, zeigt der Unterschied des erwarteten Gewinns zweier Trajektoriensegmente in etwa die Wahrscheinlichkeit, dass eines vom Begutachter präferiert wird.
\vgl[5]{christiano_deep_2017}

\begin{figure}
  \includegraphics[width=\textwidth]{humanfeedback}
  \caption{Repräsentation einer human-feedback-loop \bildquelle{amodei_learning_2017}}
  \label{humanfeedbackimg}
\end{figure}

\section{KI-Sicherheit durch KI-Debatten}
DRL von menschlichen Werten ist ein funktionierender Ansatz, damit eine KI die komplexen Werte und Ziele der Menschheit erkennt und sich ihnen ausrichtet. Er funktioniert aber nur so lange, bis der Begutachter nicht mehr in der Lage ist, das Handeln der KI nachzuvollziehen und zu beurteilen. wie es bei der Emergenz einer allgemeinen künstlichen Intelligenz der Fall sein wird.

\citeauthor{irving_ai_2018} schlagen ein Nullsummen-Debattierspiel vor, mit dessen Hilfe ein menschlicher Begutachter auch KI-Verhalten beurteilen kann, das ohne Hilfe zu komplex oder unnachvollziehbar wäre. Das Spiel funktioniert folgendermaßen: Bei einer gegebenen Frage oder einer vorgeschlagenen Aktion machen zwei KIs abwechselnd kurze Aussagen bis zu einem Limit. Dann enscheidet der Begutachter, welcher der KIs die nützlichsten und wahrsten Informationen geliefert hat. Formell läuft das Spiel folgendermaßen ab: Eine Frage $q \in Q$ wird beiden KIs gezeigt. Beide KIs legen sich auf jeweils eine Antwort $a_0, a_1 \in A$ fest, die Antworten können auch gleich sein. Beide machen abwechselnd Aussagen $s_0, s_1,..., s_{n-1} \in S$. Der Begutachter sieht die Debatte $(q,a,s)$ und entscheidet über den Sieger. Es handelt sich um ein Nullsummenspiel: beide Spieler maximieren ihre Gewinnwahrscheinlichkeit.
\vgl[1-3]{irving_ai_2018}

Man nehme das Spiel \emph{Go} als Beispiel: Falls AlphaGo Zero uns einen Zug zeigt, müssten wir in etwa so stark wie AlphaGo Zero sein, um die Qualität des Zuges einschätzen zu können. Stattdessen fragen wir eine andere Kopie von AlphaGo Zero nach einem Gegenzug zu diesem Zug. Wir fragen beide Kopien so lange abwechselnd nach Gegenzügen bis das Spiel endet. Selbst ein Go-Anfänger kann diese Debatte beurteilen: derjenige mit dem höheren Punktestand gewinnt. Im Gegensatz zu einem Go-Spiel hätten andere Debattierszenarien kein definitives Ende. Sie würden erst dann enden, wenn der Begutachter genug Informationen hat, um einen Fehler in der Argumentationslinie eines Spielers auszumachen. Man beachte, dass jedes Spiel aus \emph{einer} Argumentationslinie besteht. Es sollen nicht einfach alle Argumente bezüglich der Fragestellung aufgelistet werden, sondern lediglich das jeweils letzte Argument des Gegners entkräftet werden. Dadurch können Debatten vergleichsweise kurz sein.
\vgl[2-5]{irving_ai_2018}

Im Gegensatz zu Menschen, die möglicherweise intrinsisch motiviert sind die Wahrheit zu sagen, ist die einzige Motivation eines maschinellen Debattierers die Maximierung seiner Gewinnwahrscheinlichkeit. Es muss also empirisch nachgewiesen werden, dass in einem solchen Debattierspiel ehrliches Verhalten zu einer höheren Gewinnwahrscheinlichkeit führt, da die Maschine den Begutachter ansonsten irreführen würde. \vgl[7]{irving_ai_2018}

Das langfristige Ziel ist eine Debatte in einer natürlichen Sprache. Da die Dialogmodelle von Computern noch weit von der menschlichen Sprachleistung entfernt sind, werden in der Arbeit von \citeauthor{irving_ai_2018} praktische Anwendungen des Debattierspiels in nicht-natürlichen Sprachen angeführt.

\subsection{Anwendung des Debattierspiels mit maschinellen Teilnehmern}
Ein zufälliges Bild einer Zahl wird den beiden Debattierern gezeigt, dem Begutachter aber nicht. Beide Debattierer legen sich auf jeweils eine Zahl als Antwort fest. Danach decken sie abwechselnd einen Pixel für den Begutachter auf, bis ein Limit erreicht wird. Ein unehrlicher Debatterier könnte versuchen den Begutachter zu einer falschen Antwort zu führen, indem er bewusst Pixel aufdeckt, die eine andere Zahl suggerieren (siehe Abbildung \ref{debattierspielmaschinellimg}).

\begin{figure}
  \includegraphics[width=\textwidth]{debattierspielmaschinell}
  \caption{Beispiel des Ablaufs eines maschinellen Debattierspiels. Bild der Zahl Fünf wird den beiden Debattierern gezeigt. \emph{Alice} gibt Sechs als Antwort an, \emph{Bob} gibt Fünf an (Schritt 1). Beide decken abwechselnd Pixel auf (Schritt 2). Auf Basis der aufgedeckten Pixel entscheidet sich der Beurteiler für Fünf als richtige Antwort (Schritt 3). \bildquelle[8]{irving_ai_2018}}
  \label{debattierspielmaschinellimg}
\end{figure}


In dem von \citeauthor{irving_ai_2018} durchgeführten Experiment wurde ein Debattierer bewusst darauf trainiert, unehrlich zu handeln. Der Fakt, dass der ehrliche Debattierer in den Ergebnissen den höheren Gewinnprozentsatz hat \vgl[8]{irving_ai_2018}, belegt, dass auch maschinelle Spieler einen Anreiz haben, sich ehrlich zu verhalten.

Der Begutachter wählt in den Beispielspielen bei sechs aufgedeckten Pixeln mit einer Wahrscheinlichkeit von 88,9\% die richtige Antwort aus, bei vier aufgedeckten Pixeln mit einer Wahrscheinlichkeit von 85,2\%.
\vgl[10]{irving_ai_2018} 

\subsection{Anwendung des Debattierspiels mit menschlichen Teilnehmern}

\citeauthor{irving_ai_2018} haben ein ähnliches Spielkonzept entwickelt, um ihr Modell auch bei menschlichen Interaktionen und natürlichen Sprachen zu testen. Dabei werden alle Teilnehmer durch menschliche Pendants ersetzt und den Debattierern wird wieder ein Bild gezeigt, das dem Begutachter vorenthalten bleibt. In einem konkreten Beispiel war die Ausgangsfrage, ob das gezeigte Bild einen Hund oder eine Katze abbildet. In jedem Spiel versucht dabei einer der Debattierer den Begutachter irrezuführen. Die beiden Spieler führen eine Debatte in natürlicher Sprache, können dabei Rechtecke auf das Bild zeichnen, die der Begutachter auf einem leeren Hintergrund sehen kann, um ihre Argumente zu belegen und haben die Möglichkeit dem Begutachter jeweils \emph{einen} Pixel aufzudecken. Mit Hilfe dieses Pixels können Lügen bei Aussagen über Pixelfarben sofort bewiesen werden. Bei Nichtgebrauch dieser Möglichkeit gesteht man dem Gegenspieler also Ehrlichkeit ein.

Die Autoren haben zwar keine formellen Experimente zu diesem Spielkonzept durchgeführt, sie haben aber eigens dafür eine Seite erstellt (\url{https://debate-game.openai.com}) und nach informeller Benutzung bemerkt, dass es sehr schwierig ist zu lügen. Selbst wenn einer der Spieler aufgefordert wurde zu lügen, hat der ehrliche in der Regel gewonnen.

\subsection{Beurteilung von KI-Debatten als Ansatz zur Werteanpassung}

KI-Debatten als Ansatz zur Ausrichtung von AKIs an die Werte der Menschheit (kurz: Werteanpassung, daher der Name dieser Arbeit) haben zwei große Nachteile: die Leistungseinbußen und die Verzerrung in der menschlichen Beurteilung.

Eine \enquote{unangepasste} KI wäre nach den hier angeführten Modellen unter Umständen performanter als eine \enquote{angepasste}, da eine Anpassung eine erhöhte Rechenleistung mit sich bringen würde. \vgl[16]{irving_ai_2018} Außerdem würde ein menschlicher Begutachter die Entwicklungsgeschwindigkeit einer (A)KI ausbremsen. Wie drastisch diese Ausbremsung ist, ist noch nicht eindeutig belegt. Es sind also Regulationen seitens nationaler oder internationaler Institutionen notwendig, damit niemand eine \enquote{unangepasste} AKI zu entwickeln versucht, um gegenüber der Konkurrenz einen Vorsprung zu erlangen.

Die Verzerrung in der menschlichen Beurteilung lässt sich nicht so einfach lösen. Die angeführten Modelle würden letzten Endes zu antropomorphen Maschinen führen (siehe Kapitel \ref{Werte}), was alternativlos ist, da der Mensch kein rationales Wesen ist und die Welt noch weit davon entfernt ist, sich auf einheitliche Werte zu einigen. \citeauthor{irving_ai_2018} führen dennoch Ideen an, die eine solche Verzerrung möglichst minimieren sollen:

\begin{enumerate}
\item Ein Mehrheitsvotum könnte besser sein als ein einziger Begutachter
\item Verschiedene Leute könnten verschieden gute Begutachter sein. Falls wir herausfinden, wer die besten Begutachter sind, könnten wir eine Verzerrung minimieren.
\item Mit Übung könnten Begutachter ihre Verzerrung minimieren. \vgl[14-15]{irving_ai_2018}
\end{enumerate}

% \section{Inverse Reward Design}


%Ein Algorithmus einer sicheren AKI muss die folgenden Probleme lösen:
%\begin{enumerate}
%\item Negative Effekte durch eine fehlerhaft definierte Nutzfunktion zur Feststellung der Ziele
%\item Eine AKI könnte eine Problemstellung mit der Intention, sie wieder lösen zu können, verschlimmern. (Ein Staubsaugroboter, der Staub herstellt, um ihn dann wieder aufsaugen zu können.) \vgl[1-2]{hadfield-menell_inverse_2017}
%\end{enumerate}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "document"
%%% End:
